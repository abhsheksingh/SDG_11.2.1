{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Public Transport Accessibility Project Analysis for SDG Indicator 11.2.1 This project is to build a data pipeline to analyse data for the UN Sustainable Development Goal indicator 11.2.1, which is part of goal 11: \"Make cities and human settlements inclusive, safe, resilient and sustainable\" By assessing geographical and census data with data on public transport access points (stops and stations), an assessment can be made about the degree of acessability individuals in the population have to public transport.","title":"Home"},{"location":"#the-public-transport-accessibility-project","text":"","title":"The Public Transport Accessibility Project"},{"location":"#analysis-for-sdg-indicator-1121","text":"This project is to build a data pipeline to analyse data for the UN Sustainable Development Goal indicator 11.2.1, which is part of goal 11: \"Make cities and human settlements inclusive, safe, resilient and sustainable\" By assessing geographical and census data with data on public transport access points (stops and stations), an assessment can be made about the degree of acessability individuals in the population have to public transport.","title":"Analysis for SDG Indicator 11.2.1"},{"location":"SDG_NI/","text":"Technical documentation for the SDG_NI module. Any docstrings in this file are automatically copied to this page.","title":"SDG NI"},{"location":"SDG_bus_timetable/","text":"Technical documentation for the SDG_bus_timetable module. Any docstrings in this file are automatically copied to this page.","title":"SDG bus timetable"},{"location":"SDG_scotland/","text":"Technical documentation for the SDG_scotland module. Any docstrings in this file are automatically copied to this page.","title":"SDG scotland"},{"location":"SDG_train_timetable/","text":"Technical documentation for the SDG_train_timetable module. Any docstrings in this file are automatically copied to this page.","title":"SDG train timetable"},{"location":"about/","text":"About the ONS SDG 11.2.1 project This project has been created as side project by James Westwood, Antonio Felton, Paige Hunter and Nathan Shaw, with Musa Chirikeni as a geospatial adviser. All work on the project has been done in our self-development time on a voluntary basis. Description of Target 11.2 By 2030, provide access to safe, affordable, accessible and sustainable transport systems for all, improving road safety, notably by expanding public transport, with special attention to the needs of those in vulnerable situations, women, children, persons with disabilities and older persons Description of Indicator 11.2.1 \"Proportion of population that has convenient access to public transport, by sex, age and persons with disabilities\" Aims of the project The aims are: to build a data pipeline that can analyse data to assess the availability of public transport services for the population of the UK to make the code reusable so that it may be able to analyse other data and: assess the availability of other services across the UK be used to assess the availability of services in other nations Deliverables of the project Open source reuseable code which relies only on open source resources A new dataset with analysis of public transport availability for the UK population Three additional datasets with analysis of public transport availability disagregated by sex, age and disabilities","title":"About"},{"location":"about/#about-the-ons-sdg-1121-project","text":"This project has been created as side project by James Westwood, Antonio Felton, Paige Hunter and Nathan Shaw, with Musa Chirikeni as a geospatial adviser. All work on the project has been done in our self-development time on a voluntary basis.","title":"About the ONS SDG 11.2.1 project"},{"location":"about/#description-of-target-112","text":"By 2030, provide access to safe, affordable, accessible and sustainable transport systems for all, improving road safety, notably by expanding public transport, with special attention to the needs of those in vulnerable situations, women, children, persons with disabilities and older persons","title":"Description of Target 11.2"},{"location":"about/#description-of-indicator-1121","text":"\"Proportion of population that has convenient access to public transport, by sex, age and persons with disabilities\"","title":"Description of Indicator 11.2.1"},{"location":"about/#aims-of-the-project","text":"The aims are: to build a data pipeline that can analyse data to assess the availability of public transport services for the population of the UK to make the code reusable so that it may be able to analyse other data and: assess the availability of other services across the UK be used to assess the availability of services in other nations","title":"Aims of the project"},{"location":"about/#deliverables-of-the-project","text":"Open source reuseable code which relies only on open source resources A new dataset with analysis of public transport availability for the UK population Three additional datasets with analysis of public transport availability disagregated by sex, age and disabilities","title":"Deliverables of the project"},{"location":"building_docs/","text":"Using mkdocs to edit these pages Modifying a page If you want to edit an existing page, you need to edit the corresponding .md file which can be found in the docs/ folder of this project. Follow these instructions: Create a new branch (with the example name of my-page-edits ) using Git with the following command: git checkout -b my-page-edits Make the necessary changes to the markdown files that you want to update. Git add and commit as usual. After committing your changes, run the following command to rebuild your site: mkdocs build After running the build command, mkdocs will create a new set of HTML files in the 'site/' directory, replacing the previous version of your site. Before pushing, it's a good idea to see the changes on your local machine. To preview your changes, you can run the following command to launch a local server: mkdocs serve Open your web browser and navigate to http://localhost:8000 to see your changes in action. If you are happy with your changes, you can push your updated branch to your Git repository. After that create a pull request to main and the Github Action (controlled by this yaml .github/workflows/pages.yml ) should trigger the re-building and deployment of the pages. Manually deploying pages with gh-deploy You can deploy any changes you have made locally using mkdocs gh-deploy . Look for more guidance in the mkdocs official documentation Repository \"Pages\" settings You should select the \"gh-pages\" branch as the source branch in the Pages section of Github's Pages settings. Here are the steps to follow: Go to your Github repository's settings page. Scroll down to the \"Pages\" section. Under \"Source\", select \"gh-pages\" from the dropdown menu. Click \"Save\". This tells Github to serve your pages from the \"gh-pages\" branch of your repository, which is where mkdocs gh-deploy will push your documentation. Note that it may take a few minutes for your changes to take effect on the Github Pages site.","title":"Using mkdocs to edit these pages"},{"location":"building_docs/#using-mkdocs-to-edit-these-pages","text":"","title":"Using mkdocs to edit these pages"},{"location":"building_docs/#modifying-a-page","text":"If you want to edit an existing page, you need to edit the corresponding .md file which can be found in the docs/ folder of this project. Follow these instructions: Create a new branch (with the example name of my-page-edits ) using Git with the following command: git checkout -b my-page-edits Make the necessary changes to the markdown files that you want to update. Git add and commit as usual. After committing your changes, run the following command to rebuild your site: mkdocs build After running the build command, mkdocs will create a new set of HTML files in the 'site/' directory, replacing the previous version of your site. Before pushing, it's a good idea to see the changes on your local machine. To preview your changes, you can run the following command to launch a local server: mkdocs serve Open your web browser and navigate to http://localhost:8000 to see your changes in action. If you are happy with your changes, you can push your updated branch to your Git repository. After that create a pull request to main and the Github Action (controlled by this yaml .github/workflows/pages.yml ) should trigger the re-building and deployment of the pages.","title":"Modifying a page"},{"location":"building_docs/#manually-deploying-pages-with-gh-deploy","text":"You can deploy any changes you have made locally using mkdocs gh-deploy . Look for more guidance in the mkdocs official documentation","title":"Manually deploying pages with gh-deploy"},{"location":"building_docs/#repository-pages-settings","text":"You should select the \"gh-pages\" branch as the source branch in the Pages section of Github's Pages settings. Here are the steps to follow: Go to your Github repository's settings page. Scroll down to the \"Pages\" section. Under \"Source\", select \"gh-pages\" from the dropdown menu. Click \"Save\". This tells Github to serve your pages from the \"gh-pages\" branch of your repository, which is where mkdocs gh-deploy will push your documentation. Note that it may take a few minutes for your changes to take effect on the Github Pages site.","title":"Repository \"Pages\" settings"},{"location":"comparison_to_EU_method/","text":"Calculation of Statistics for SDG 11.2.1: The UK Method Compared to the EU Method Differences and similarities between our method and that of the EU The following is a comparison of the method employed to calculate public transport accessibility by a team researching for the EU Commission and a UK team, working on behalf of the Sustainable Development Goals team in the Office for National Statistics (ONS). The two teams will be referred to as the \u201cONS team\u201d and the \u201cEU team\u201d going forward. Technology used Open source development The SDG team at ONS signed up to the Inclusive Data Charter 1 . In line with its principles, those of the SDGs and OSAT we have worked as transparently as possible, making all code open source and conversations around method and development decisions published on our Github repository, and used free and open source tools and technology throughout the development process. The EU team have made their code available (see annexes here ) as well as a write up about their chosen method and results, though an open development process was not found, though it may exist. Software The EU team and the ONS 11.2.1 project team have approached the challenges of calculating transport accessibility similarly, both in the methodology and the technology involved. Both teams have made use of Python as a means to ingest and process data. However the main processing of the data is done using different technologies, namely that the EU team has used the ArcPy library to interact with ArcGIS, a program which is not open source or free. The ONS team, by comparison, carries out the data processing in Pandas and the Geospatial analysis in GeoPandas , all of which are open source and free. Computation method The ONS computation calculates the service accessibility (see definitions ) for the whole country - except that currently the script cycles through local authorities and aggregates the data into a single table later. This method of calculating areas and combining up to the whole nation later will possibly be removed when the ONS team optimise the script. The ONS pipeline calculates the service accessibility for the whole country by cycling through local authorities and aggregating at the end of the process (for version 1.0 of our pipeline). Version 1.0 will only use data for 2011 and 1.1 will make the calculation over multiple years. The type of computation may be changed in future iterations of the project to a more vectorised computation (probably in version 1.3) - if this feature is implemented. The EU team calculates the transport service accessibility for each \u201carea of interest\u201d (e.g. city, region,urban centre, etc.) focusing on the urban area (in line with 11.2.1 methodology) but the ONS team has calculated for rural areas too - output data that is then compared with urban areas to get an idea of comparative transport access. The methodology for the calculation used The method used for the calculation is also very similar, but there are some important differences. Transport access points The data on the location of the transport nodes (stops and stations) is required for this calculation. The ONS team sourced the data from the National Public Transport Access Nodes (NaPTAN) data set. It is not clear where the EU team sourced their data from. The EU team cluster stops together if they are within 50 metres of each other. The cluster is then represented by a single point, mid distance between the stops that have been clustered. On the other hand, the ONS team treats every stop in the country as individual stops, each of which is treated as an individual point. To establish the effect on the results this would have, our team would have to update our code to cluster stops in the same way, a step the ONS team will consider for future iterations of the project. Public transport service areas When it comes to calculating the service area, both teams use the same distance as a buffer. The difference is how the distance from the central node (station/stop/cluster) is calculated. In the EU team\u2019s method, a service area (catchment areas) of 500 metres or 1000m (according to capacity) is created. This is made using a pedestrian path and road network. The ONS team use a simple Euclidean buffer to create a circle (techinically a polygon) around each stop. The polygons are then combined to create the entire service area. Our team has considered the network/path method, and it has not been ruled out, however the ONS team note that computationally this will be more intensive and accurate results rely on a very accurate pedestrian path network. If the network is incomplete, or maps paths on to pedestrian unfriendly roads (such as fast roads with no pavements) the results will be unreliable. Examining the EU method, by using the road network, based on the shape_length parameter (which is probably the path line) and the walking speed, an isochrone of the service area is created. This is almost certainly how the above-mentioned service area polygons are created. The current ONS method makes no use of isochrones or irregular polygons in the calculation of service areas. Calculation of served vs. unserved population Seemingly the method of counting the population inside and outside of the service areas is the same on both teams, though the method of establishing the population in each area is different. Both the ONS team and the EU team count the population inside and outside of the service areas. The ONS team use sum method in Pandas dataframe, after running a points in polygons query (a kind of spatial join) to only include population centres that are within the population centre. Differentiating high and low capacity stops The ONS team have differentiated high and low capacity stops using the StopType field in the NaPTAN dataset, the feasibility and methodology of which was discussed here and here ; and the implementation of these features are discussed here and here . The EU team imports two datasets of high and low capacity stops. However it is unclear how those datasets are generated. The ONS team group underground service (in London) at the same level as Trams (e.g in Manchester), however this may change based on further discussion before the release of version 1.0. Similarities and differences in our data Spatial granularity of data The ONS team have used population data at output area level, which is the most granular level available. The Government Statistical Service states that the 2011 Census, England was divided into 171,372 Output Areas (OAs) which on average have a resident population of 309 people. According to their publication the EU team population estimate figures at the building-block level and combine that data at the best available spatial resolution with data on land cover, land use, and data on the location, function and height of buildings to obtain estimates of a useful quality. Urban and rural definitions The ONS definition of \"Urban\" and \"Rural\" is defined by their population density in each output area. According to Government Statistical Service, urban areas are the connected built up areas identified by Ordnance Survey mapping that have resident populations \\ above 10,000 people (2011 Census). On the other hand rural areas are those areas that are not urban, i.e. consisting of settlements below 10,000 people or are open countryside. The EU team have used the EU definition of urban, which states that urban centres have a population density of more than 1 500 inhabitants/km\u00b2.\u201d (Poelman et al., pg. 30). Additionally they use the EU-OECD \u201cFunctional Urban Area\u201d definition for urban centres, for which is the conglomerate they calculate. Disaggregations Age and sex The ONS team disaggregates on age and sex. Having used the UK census population data which is detailed all ages and sex of population at output level up to 90+. The population data was then \u201cbinned\u201d into 5 year groups based on a discussion with the SDG data team noted here (Nov, 2020). The binning can be changed as it is defined in the config file; though this will be improved and fully tested in a future iteration . The EU team intends to use the European population grid for 2021 for disaggregated breakdowns, but in the most recent publication , they did not carry out disaggregated analysis by age, nor sex. Disability status NOMIS is a research database and analytical tool that provides data on labour markets, demographics, and the economy. NOMIS stands for \"New Opportunities for Migrants' Integration and Success\" and is funded by the Economic and Social Research Council (ESRC) in the United Kingdom. Via NOMIS , data on disability status output from the UK census data is available for the year of the census. For years later than the census, the population estimate for each area was multiplied by the proportion of each disability category, calculated from the census year (see the methodology writeup on disability ). Again, the EU team did not carry out disaggregated analysis on disability across the EU due to a lack of data. Notes Inclusive data charter action plan for the global Sustainable Development Goals \u21a9","title":"EU and UK Methodology Comparison"},{"location":"comparison_to_EU_method/#calculation-of-statistics-for-sdg-1121-the-uk-method-compared-to-the-eu-method","text":"","title":"Calculation of Statistics for SDG 11.2.1: The UK Method Compared to the EU Method"},{"location":"comparison_to_EU_method/#differences-and-similarities-between-our-method-and-that-of-the-eu","text":"The following is a comparison of the method employed to calculate public transport accessibility by a team researching for the EU Commission and a UK team, working on behalf of the Sustainable Development Goals team in the Office for National Statistics (ONS). The two teams will be referred to as the \u201cONS team\u201d and the \u201cEU team\u201d going forward.","title":"Differences and similarities between our method and that of the EU"},{"location":"comparison_to_EU_method/#technology-used","text":"","title":"Technology used"},{"location":"comparison_to_EU_method/#open-source-development","text":"The SDG team at ONS signed up to the Inclusive Data Charter 1 . In line with its principles, those of the SDGs and OSAT we have worked as transparently as possible, making all code open source and conversations around method and development decisions published on our Github repository, and used free and open source tools and technology throughout the development process. The EU team have made their code available (see annexes here ) as well as a write up about their chosen method and results, though an open development process was not found, though it may exist.","title":"Open source development"},{"location":"comparison_to_EU_method/#software","text":"The EU team and the ONS 11.2.1 project team have approached the challenges of calculating transport accessibility similarly, both in the methodology and the technology involved. Both teams have made use of Python as a means to ingest and process data. However the main processing of the data is done using different technologies, namely that the EU team has used the ArcPy library to interact with ArcGIS, a program which is not open source or free. The ONS team, by comparison, carries out the data processing in Pandas and the Geospatial analysis in GeoPandas , all of which are open source and free.","title":"Software"},{"location":"comparison_to_EU_method/#computation-method","text":"The ONS computation calculates the service accessibility (see definitions ) for the whole country - except that currently the script cycles through local authorities and aggregates the data into a single table later. This method of calculating areas and combining up to the whole nation later will possibly be removed when the ONS team optimise the script. The ONS pipeline calculates the service accessibility for the whole country by cycling through local authorities and aggregating at the end of the process (for version 1.0 of our pipeline). Version 1.0 will only use data for 2011 and 1.1 will make the calculation over multiple years. The type of computation may be changed in future iterations of the project to a more vectorised computation (probably in version 1.3) - if this feature is implemented. The EU team calculates the transport service accessibility for each \u201carea of interest\u201d (e.g. city, region,urban centre, etc.) focusing on the urban area (in line with 11.2.1 methodology) but the ONS team has calculated for rural areas too - output data that is then compared with urban areas to get an idea of comparative transport access.","title":"Computation method"},{"location":"comparison_to_EU_method/#the-methodology-for-the-calculation-used","text":"The method used for the calculation is also very similar, but there are some important differences.","title":"The methodology for the calculation used"},{"location":"comparison_to_EU_method/#transport-access-points","text":"The data on the location of the transport nodes (stops and stations) is required for this calculation. The ONS team sourced the data from the National Public Transport Access Nodes (NaPTAN) data set. It is not clear where the EU team sourced their data from. The EU team cluster stops together if they are within 50 metres of each other. The cluster is then represented by a single point, mid distance between the stops that have been clustered. On the other hand, the ONS team treats every stop in the country as individual stops, each of which is treated as an individual point. To establish the effect on the results this would have, our team would have to update our code to cluster stops in the same way, a step the ONS team will consider for future iterations of the project.","title":"Transport access points"},{"location":"comparison_to_EU_method/#public-transport-service-areas","text":"When it comes to calculating the service area, both teams use the same distance as a buffer. The difference is how the distance from the central node (station/stop/cluster) is calculated. In the EU team\u2019s method, a service area (catchment areas) of 500 metres or 1000m (according to capacity) is created. This is made using a pedestrian path and road network. The ONS team use a simple Euclidean buffer to create a circle (techinically a polygon) around each stop. The polygons are then combined to create the entire service area. Our team has considered the network/path method, and it has not been ruled out, however the ONS team note that computationally this will be more intensive and accurate results rely on a very accurate pedestrian path network. If the network is incomplete, or maps paths on to pedestrian unfriendly roads (such as fast roads with no pavements) the results will be unreliable. Examining the EU method, by using the road network, based on the shape_length parameter (which is probably the path line) and the walking speed, an isochrone of the service area is created. This is almost certainly how the above-mentioned service area polygons are created. The current ONS method makes no use of isochrones or irregular polygons in the calculation of service areas.","title":"Public transport service areas"},{"location":"comparison_to_EU_method/#calculation-of-served-vs-unserved-population","text":"Seemingly the method of counting the population inside and outside of the service areas is the same on both teams, though the method of establishing the population in each area is different. Both the ONS team and the EU team count the population inside and outside of the service areas. The ONS team use sum method in Pandas dataframe, after running a points in polygons query (a kind of spatial join) to only include population centres that are within the population centre.","title":"Calculation of served vs. unserved population"},{"location":"comparison_to_EU_method/#differentiating-high-and-low-capacity-stops","text":"The ONS team have differentiated high and low capacity stops using the StopType field in the NaPTAN dataset, the feasibility and methodology of which was discussed here and here ; and the implementation of these features are discussed here and here . The EU team imports two datasets of high and low capacity stops. However it is unclear how those datasets are generated. The ONS team group underground service (in London) at the same level as Trams (e.g in Manchester), however this may change based on further discussion before the release of version 1.0.","title":"Differentiating high and low capacity stops"},{"location":"comparison_to_EU_method/#similarities-and-differences-in-our-data","text":"","title":"Similarities and differences in our data"},{"location":"comparison_to_EU_method/#spatial-granularity-of-data","text":"The ONS team have used population data at output area level, which is the most granular level available. The Government Statistical Service states that the 2011 Census, England was divided into 171,372 Output Areas (OAs) which on average have a resident population of 309 people. According to their publication the EU team population estimate figures at the building-block level and combine that data at the best available spatial resolution with data on land cover, land use, and data on the location, function and height of buildings to obtain estimates of a useful quality.","title":"Spatial granularity of data"},{"location":"comparison_to_EU_method/#urban-and-rural-definitions","text":"The ONS definition of \"Urban\" and \"Rural\" is defined by their population density in each output area. According to Government Statistical Service, urban areas are the connected built up areas identified by Ordnance Survey mapping that have resident populations \\ above 10,000 people (2011 Census). On the other hand rural areas are those areas that are not urban, i.e. consisting of settlements below 10,000 people or are open countryside. The EU team have used the EU definition of urban, which states that urban centres have a population density of more than 1 500 inhabitants/km\u00b2.\u201d (Poelman et al., pg. 30). Additionally they use the EU-OECD \u201cFunctional Urban Area\u201d definition for urban centres, for which is the conglomerate they calculate.","title":"Urban and rural definitions"},{"location":"comparison_to_EU_method/#disaggregations","text":"Age and sex The ONS team disaggregates on age and sex. Having used the UK census population data which is detailed all ages and sex of population at output level up to 90+. The population data was then \u201cbinned\u201d into 5 year groups based on a discussion with the SDG data team noted here (Nov, 2020). The binning can be changed as it is defined in the config file; though this will be improved and fully tested in a future iteration . The EU team intends to use the European population grid for 2021 for disaggregated breakdowns, but in the most recent publication , they did not carry out disaggregated analysis by age, nor sex. Disability status NOMIS is a research database and analytical tool that provides data on labour markets, demographics, and the economy. NOMIS stands for \"New Opportunities for Migrants' Integration and Success\" and is funded by the Economic and Social Research Council (ESRC) in the United Kingdom. Via NOMIS , data on disability status output from the UK census data is available for the year of the census. For years later than the census, the population estimate for each area was multiplied by the proportion of each disability category, calculated from the census year (see the methodology writeup on disability ). Again, the EU team did not carry out disaggregated analysis on disability across the EU due to a lack of data.","title":"Disaggregations"},{"location":"comparison_to_EU_method/#notes","text":"Inclusive data charter action plan for the global Sustainable Development Goals \u21a9","title":"Notes"},{"location":"data_description/","text":"Data The Public Transport Availability project looks to assess the proportion of people who live near a public transport stop. Below is a description of the data sources used in order to perform this calculation. NaPTAN The National Public Transport Access Nodes (NaPTAN) dataset contains a list of all public transport access points in Great Britain including bus, rail and tram. This is open-source data and is publicly available. As of 3rd May 2022, the dataset includes around 435,000 public transport access points. The following columns are used within our calculations. The full schema for NapTAN can be found here . Column Description NaptanCode Seven- or eight-digit identifier for access point. CommonName Name of bus stop. Easting Uk Geocoding reference. Northing Status Whether an access point is active, inactive or pending. StopType The type of access point e.g bus or rail The dataset is filtered based on two conditions. The Status column must not be inactive. This ensures that historic public transport access points are not included in the calculations. The StopType is either a bus or rail public transport access point. Derived Variables The StopType that are included are in the calculations are \"RSE\", \"RLY\", \"RPL\", \"TMU\", \"MET\", \"PLT\", \"BCE\", \"BST\",\"BCQ\", \"BCS\",\"BCT\". After filtering there are 383,662 public transport access points. A capacity_type variable is derived which classifies public transport as either high or low capacity. This is consistent with the UN definition . A geometry variable is derived which creates a polygon around each public transport access point. The polygon for a low capacity bus stop is a circle with radius of 500 metres with the access point being the centre point. The polygon for high capacity is the same with a circle with a radius of 1000 metres. These polygons will be used to determine if a weighted centroid lives within the polygon. Census Data The census takes place every 10 years and aims to obtain information on all households in the UK and statistics are published at various geographic levels. Output area (OA) is a lower level geography which contains on average approximately 300 people. For the purposes of our calculations each OA will be grouped together into one household. Census data is used to calculate percentages of certain demographics which can then be applied to the annual population estimates. For example the annual population estimates do not include information on disability status. A proportion of people who are disabed can be calculated from the 2011 Census per OA and then applied to the population estimates data. The Population Weighted Centroids for OA from the 2011 census are used. These are OA\u2019s containing where the midpoint of their population is. These are the points used to deduce whether an OA is contained within a service area. The Urban/Rural Classification is used to classify if an OA is urban or rural. This is used to be able to calculate different estimates for each classification. The OA\u2019s are classed as \u2018urban\u2019 if they were allocated to a 2011 built-up area with a population of 10,000 people or more. All other remaining OAs are classed as \u2018rural\u2019. The QS303UK - Long-term health problem or disability dataset, derived from the 2011 census, contains disability information on a OA basis. This information is transformed to be consistent with the GSS harmonized disability data . This allows us to produce estimates disaggregated by disability status. Annual Population Estimates The ONS produces population estimates every year. This contains information on population estimates for OA\u2019s. This also contains splits of age and sex. These are annual so the year used is consistent with the calculation year. Local Authorities (LA) Boundary The boundaries of each local authorities are used to ensure calculations are aggregated to an LA basis. The lookup file between LA and OA is important when aggregating OA estimates to produce the LA figure. These boundaries and lookup files are annual so the year used is consistent with the calculation year.","title":"Data Description"},{"location":"data_description/#data","text":"The Public Transport Availability project looks to assess the proportion of people who live near a public transport stop. Below is a description of the data sources used in order to perform this calculation.","title":"Data"},{"location":"data_description/#naptan","text":"The National Public Transport Access Nodes (NaPTAN) dataset contains a list of all public transport access points in Great Britain including bus, rail and tram. This is open-source data and is publicly available. As of 3rd May 2022, the dataset includes around 435,000 public transport access points. The following columns are used within our calculations. The full schema for NapTAN can be found here . Column Description NaptanCode Seven- or eight-digit identifier for access point. CommonName Name of bus stop. Easting Uk Geocoding reference. Northing Status Whether an access point is active, inactive or pending. StopType The type of access point e.g bus or rail The dataset is filtered based on two conditions. The Status column must not be inactive. This ensures that historic public transport access points are not included in the calculations. The StopType is either a bus or rail public transport access point.","title":"NaPTAN"},{"location":"data_description/#derived-variables","text":"The StopType that are included are in the calculations are \"RSE\", \"RLY\", \"RPL\", \"TMU\", \"MET\", \"PLT\", \"BCE\", \"BST\",\"BCQ\", \"BCS\",\"BCT\". After filtering there are 383,662 public transport access points. A capacity_type variable is derived which classifies public transport as either high or low capacity. This is consistent with the UN definition . A geometry variable is derived which creates a polygon around each public transport access point. The polygon for a low capacity bus stop is a circle with radius of 500 metres with the access point being the centre point. The polygon for high capacity is the same with a circle with a radius of 1000 metres. These polygons will be used to determine if a weighted centroid lives within the polygon.","title":"Derived Variables"},{"location":"data_description/#census-data","text":"The census takes place every 10 years and aims to obtain information on all households in the UK and statistics are published at various geographic levels. Output area (OA) is a lower level geography which contains on average approximately 300 people. For the purposes of our calculations each OA will be grouped together into one household. Census data is used to calculate percentages of certain demographics which can then be applied to the annual population estimates. For example the annual population estimates do not include information on disability status. A proportion of people who are disabed can be calculated from the 2011 Census per OA and then applied to the population estimates data. The Population Weighted Centroids for OA from the 2011 census are used. These are OA\u2019s containing where the midpoint of their population is. These are the points used to deduce whether an OA is contained within a service area. The Urban/Rural Classification is used to classify if an OA is urban or rural. This is used to be able to calculate different estimates for each classification. The OA\u2019s are classed as \u2018urban\u2019 if they were allocated to a 2011 built-up area with a population of 10,000 people or more. All other remaining OAs are classed as \u2018rural\u2019. The QS303UK - Long-term health problem or disability dataset, derived from the 2011 census, contains disability information on a OA basis. This information is transformed to be consistent with the GSS harmonized disability data . This allows us to produce estimates disaggregated by disability status.","title":"Census Data"},{"location":"data_description/#annual-population-estimates","text":"The ONS produces population estimates every year. This contains information on population estimates for OA\u2019s. This also contains splits of age and sex. These are annual so the year used is consistent with the calculation year.","title":"Annual Population Estimates"},{"location":"data_description/#local-authorities-la-boundary","text":"The boundaries of each local authorities are used to ensure calculations are aggregated to an LA basis. The lookup file between LA and OA is important when aggregating OA estimates to produce the LA figure. These boundaries and lookup files are annual so the year used is consistent with the calculation year.","title":"Local Authorities (LA) Boundary"},{"location":"data_ingest/","text":"All functions and classes related to ingesting data into the pipeline any_to_pd ( file_nm , zip_link , ext_order , dtypes , data_dir = DATA_DIR ) A function which ties together many other data ingest related functions to import data. Currently this function can handle the remote or local import of data from zip (containing csv files), and csv files. The main purpose is to check for locally stored persistent data files and get that data into a dataframe for further processing. Each time the function checks for download/extracted data so it doesn't have to go through the process again. Firstly the function checks for a feather file and loads that if available. If the feather is not available the function checks for a csv file, then loads that if available. If no csv file is available it checks for a zip file, from which to extract the csv from. If a zip file is not available locally it falls back to downloading the zip file (which could contains other un-needed datasets) then extracts the specified/needed data set (csv) and deletes the now un-needed zip file. This function should be used in place of pd.read_csv for example. Parameters: file_nm ( str ) \u2013 The name of the file without extension. zip_link ( str ) \u2013 URL containing zipped data. ext_order ( list ) \u2013 The order in which to try extraction methods. dtypes ( Optional [ Dict ] ) \u2013 Datatypes of columns in the csv. Returns: pd . DataFrame \u2013 pd.DataFrame: A dataframe of the data that has been imported. src/data_ingest.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def any_to_pd ( file_nm : str , zip_link : str , ext_order : List , dtypes : Optional [ Dict ], data_dir = DATA_DIR ) -> pd . DataFrame : \"\"\"A function which ties together many other data ingest related functions to import data. Currently this function can handle the remote or local import of data from zip (containing csv files), and csv files. The main purpose is to check for locally stored persistent data files and get that data into a dataframe for further processing. Each time the function checks for download/extracted data so it doesn't have to go through the process again. Firstly the function checks for a feather file and loads that if available. If the feather is not available the function checks for a csv file, then loads that if available. If no csv file is available it checks for a zip file, from which to extract the csv from. If a zip file is not available locally it falls back to downloading the zip file (which could contains other un-needed datasets) then extracts the specified/needed data set (csv) and deletes the now un-needed zip file. This function should be used in place of pd.read_csv for example. Parameters: file_nm (str): The name of the file without extension. zip_link (str): URL containing zipped data. ext_order (list): The order in which to try extraction methods. dtypes (Optional[Dict]): Datatypes of columns in the csv. Returns: pd.DataFrame: A dataframe of the data that has been imported. \"\"\" # Change directory into project root os . chdir ( CWD ) # Make the load order (lists are ordered) to prioritise load_order = [ f \" { file_nm } . { ext } \" for ext in ext_order ] # make a list of functions that apply to these files load_funcs = { \"feather\" : _feath_to_df , \"csv\" : _csv_to_df , \"zip\" : _import_extract_delete_zip } # create a dictionary ready to dispatch functions # load_func_dict = {f\"{file_name}\": load_func # Iterate through files that might exist for i in range ( len ( load_order )): # Indexing with i because the list loading in the wrong order data_file_nm = load_order [ i ] data_file_path = _make_data_path ( data_dir , data_file_nm ) if _persistent_exists ( data_file_path ): # Check if each persistent file exists # load the persistent file by dispatching the correct function if dtypes and ext_order [ i ] == \"csv\" : pd_df = load_funcs [ ext_order [ i ]]( file_nm , data_file_path , dtypes = dtypes ) else : pd_df = load_funcs [ ext_order [ i ]]( file_nm , data_file_path ) return pd_df continue # None of the persistent files has been found. # Continue onto the next file type # A zip must be downloaded, extracted, and turned into pd_df pd_df = load_funcs [ ext_order [ i ]]( file_nm , data_file_path , persistent_exists = False , zip_url = zip_link , dtypes = dtypes ) return pd_df best_before ( path , number_of_days ) Checks whether a path has been modified within a period of days. Parameters: path ( str ) \u2013 the path to check number_of_days ( int ) \u2013 number of days previous to check for modifications Returns: Bool \u2013 True if path has not been modified within the number of days specified src/data_ingest.py 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 def best_before ( path , number_of_days ): \"\"\" Checks whether a path has been modified within a period of days. Args: path (str): the path to check number_of_days (int): number of days previous to check for modifications Returns: Bool: True if path has not been modified within the number of days specified \"\"\" todays_date = datetime . today () last_modified_date = datetime . fromtimestamp ( os . path . getmtime ( path )) days_since_last_modification = ( todays_date - last_modified_date ) . days if days_since_last_modification > number_of_days : expired = True else : expired = False return expired capture_region ( file_nm ) Extracts the region name from the ONS population estimate excel files. Parameters: file_nm ( str ) \u2013 Full name of the regional population Excel file. Returns: str \u2013 The name of the region that the file covers src/data_ingest.py 386 387 388 389 390 391 392 393 394 395 396 397 398 def capture_region ( file_nm : str ): \"\"\"Extracts the region name from the ONS population estimate excel files. Args: file_nm (str): Full name of the regional population Excel file. Returns: str: The name of the region that the file covers \"\"\" patt = re . compile ( \"^(.*estimates[-]?)(?P<region>.*)(\\.xls)\" ) region = re . search ( patt , file_nm ) . group ( \"region\" ) region = region . replace ( \"-\" , \" \" ) . capitalize () return region geo_df_from_geospatialfile ( path_to_file , crs = 'epsg:27700' ) Function to create a Geo-dataframe from a geospatial (geojson, shp) file. The process goes via Pandas.s Parameters: path_to_file ( str ) \u2013 path to the geojson, shp and other geospatial data files. Returns: \u2013 Geopandas Dataframe src/data_ingest.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 def geo_df_from_geospatialfile ( path_to_file , crs = 'epsg:27700' ): \"\"\"Function to create a Geo-dataframe from a geospatial (geojson, shp) file. The process goes via Pandas.s Args: path_to_file (str): path to the geojson, shp and other geospatial data files. Returns: Geopandas Dataframe \"\"\" geo_df = gpd . read_file ( path_to_file ) if geo_df . crs is None : geo_df . crs = 'epsg:27700' elif geo_df . crs != crs : geo_df = geo_df . to_crs ( 'epsg:27700' ) return geo_df geo_df_from_pd_df ( pd_df , geom_x , geom_y , crs ) Function to create a Geo-dataframe from a Pandas DataFrame. Parameters: pd_df ( pd . DataFrame ) \u2013 a pandas dataframe object to be converted. geom_x ( str ) \u2013 name of the column that contains the longitude data. geom_y ( str ) \u2013 name of the column that contains the latitude data. crs ( str ) \u2013 the coordinate reference system required. Returns: \u2013 Geopandas Dataframe src/data_ingest.py 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def geo_df_from_pd_df ( pd_df , geom_x , geom_y , crs ): \"\"\"Function to create a Geo-dataframe from a Pandas DataFrame. Arguments: pd_df (pd.DataFrame): a pandas dataframe object to be converted. geom_x (str):name of the column that contains the longitude data. geom_y (str):name of the column that contains the latitude data. crs (str): the coordinate reference system required. Returns: Geopandas Dataframe \"\"\" geometry = [ Point ( xy ) for xy in zip ( pd_df [ geom_x ], pd_df [ geom_y ])] geo_df = gpd . GeoDataFrame ( pd_df , geometry = geometry ) geo_df . crs = crs geo_df . to_crs ( 'EPSG:27700' , inplace = True ) return geo_df get_and_save_geo_dataset ( url , localpath , filename ) Fetches a geodataset in json format from a web resource and saves it to the local data/ directory and returns the json as a dict into memory. Parameters: filename ( str ) \u2013 the name of file as it should be saved locally. url ( str ) \u2013 URL of the web resource where json file is hosted. localpath ( str ) \u2013 path to folder where json is to be saved locally. Returns: \u2013 dict src/data_ingest.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 def get_and_save_geo_dataset ( url , localpath , filename ): \"\"\"Fetches a geodataset in json format from a web resource and saves it to the local data/ directory and returns the json as a dict into memory. Args: filename (str): the name of file as it should be saved locally. url (str): URL of the web resource where json file is hosted. localpath (str): path to folder where json is to be saved locally. Returns: dict \"\"\" file = requests . get ( url ) . json () full_path = os . path . join ( localpath , filename ) with open ( full_path , 'w' ) as dset : json . dump ( file , dset ) return file get_oa_la_csv_abspath ( dir ) Takes a directory as str and returns the absolute path of output area csv file. Parameters: dir ( str ) \u2013 Path created with os.path.join. Returns: str \u2013 Absolute path of the csv file of the Output area. src/data_ingest.py 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 def get_oa_la_csv_abspath ( dir ): \"\"\"Takes a directory as str and returns the absolute path of output area csv file. Args: dir (str): Path created with os.path.join. Returns: str: Absolute path of the csv file of the Output area. \"\"\" # Add check that the directory exists and is not empty if not os . path . exists ( dir ) or len ( os . listdir ( dir )) == 0 : raise ValueError ( f \"Directory { dir } does not exist or is empty\" ) files = os . listdir ( dir ) csv_files = [ file for file in files if file . endswith ( \".csv\" )] csv_file = csv_files [ 0 ] absolute_path = os . path . join ( dir , csv_file ) return absolute_path get_shp_abs_path ( dir ) Passed a directory into the function and returns the absolute path of where the shp file is within that directory. Parameters: dir ( PathLike ) \u2013 /path/to/directory/of/shape_files. Returns: str \u2013 the absolute path of the .shp file within a directory. src/data_ingest.py 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 def get_shp_abs_path ( dir ): \"\"\"Passed a directory into the function and returns the absolute path of where the shp file is within that directory. Args: dir (PathLike): /path/to/directory/of/shape_files. Returns: str: the absolute path of the .shp file within a directory. \"\"\" files = os . listdir ( dir ) shp_files = [ file for file in files if file . endswith ( \".shp\" )] # Add warning if there isn't a shp file in the directory if len ( shp_files ) == 0 : raise ValueError ( f \"No .shp file in directory { dir } \" ) shp_file = shp_files [ 0 ] absolute_path = os . path . join ( dir , shp_file ) return absolute_path get_stops_file ( url , dir ) Gets the latest stop dataset. If the latest stop df from the api is older then 28 days then function grabs a new version of file from API and saves this as a feather file. If the latest stop df from the api is less then 28 days old then just grabs the feather file. Parameters: url ( str ) \u2013 NAPTAN API url. dir ( str ) \u2013 directory where the stop data is stored. Returns: \u2013 pd.DataFrame src/data_ingest.py 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 def get_stops_file ( url , dir ): \"\"\"Gets the latest stop dataset. If the latest stop df from the api is older then 28 days then function grabs a new version of file from API and saves this as a feather file. If the latest stop df from the api is less then 28 days old then just grabs the feather file. Args: url (str): NAPTAN API url. dir (str): directory where the stop data is stored. Returns: pd.DataFrame \"\"\" # gets todays date and latest date of stops df today = int ( datetime . now () . strftime ( '%Y%m %d ' )) # gets feather stop path feather_path = os . path . join ( os . getcwd (), \"data\" , \"stops\" , \"Stops.feather\" ) # Check that the feather exists if not _persistent_exists ( feather_path ): stops_df = _dl_stops_make_df ( today , url ) else : # does exist latest_date = _get_latest_stop_file_date ( dir ) if today - latest_date < 28 : stops_df = pd . read_feather ( feather_path ) else : stops_df = _dl_stops_make_df ( today , url ) return stops_df get_whole_nation_pop_df ( pop_files , pop_year ) Gets the population data for all regions in the country and puts them into one dataframe. Parameters: pop_files ( list ) \u2013 Population data to be unioned. pop_year ( str ) \u2013 The year of population estimation data to process. Returns: \u2013 pd.DataFrame: Dataframe of population data for all regions in the country src/data_ingest.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 def get_whole_nation_pop_df ( pop_files , pop_year ): \"\"\"Gets the population data for all regions in the country and puts them into one dataframe. Args: pop_files (list): Population data to be unioned. pop_year (str): The year of population estimation data to process. Returns: pd.DataFrame: Dataframe of population data for all regions in the country \"\"\" # Remove gitkeep file from list of pop files pop_files = [ f for f in pop_files if f != '.gitkeep' ] # Dict of region:file_name. Capture the region name from the filename region_dict = { capture_region ( file ): file for file in pop_files } # make a df of each region then concat national_pop_feather_path = os . path . join ( DATA_DIR , f \"whole_nation_ { pop_year } .feather\" ) if not os . path . exists ( national_pop_feather_path ): print ( f \"No national_pop_feather found for { pop_year } \" ) print ( f \"Rebuilding population file. This will take a while!\" ) region_dfs_dict = {} for region in region_dict : print ( f \"Reading { region } Excel file\" ) xls_path = os . path . join ( DATA_DIR , \"population_estimates\" , str ( pop_year ), region_dict [ region ]) # Read Excel file as object xlFile = pd . ExcelFile ( xls_path ) # Access sheets in Excel file total_pop = pd . read_excel ( xlFile , f \"Mid- { pop_year } Persons\" , header = 4 ) males_pop = pd . read_excel ( xlFile , f \"Mid- { pop_year } Males\" , header = 4 , usecols = [ \"OA11CD\" , \"LSOA11CD\" , \"All Ages\" ]) fem_pop = pd . read_excel ( xlFile , f \"Mid- { pop_year } Females\" , header = 4 , usecols = [ \"OA11CD\" , \"LSOA11CD\" , \"All Ages\" ]) # Rename the \"All Ages\" columns appropriately before concating total_pop . rename ( columns = { \"All Ages\" : \"pop_count\" }, inplace = True ) males_pop . rename ( columns = { \"All Ages\" : \"males_pop\" }, inplace = True ) fem_pop . rename ( columns = { \"All Ages\" : \"fem_pop\" }, inplace = True ) # Merge the data from different sheets dfs_to_merge = [ total_pop , males_pop , fem_pop ] df_final = reduce ( lambda left , right : pd . merge ( left , right , on = 'OA11CD' ), dfs_to_merge ) # Store merged df in dict under region name region_dfs_dict [ region ] = df_final # Concat all regions into national pop df whole_nation_pop_df = pd . concat ( region_dfs_dict . values ()) # Create the pop_year column to show which year the data is from whole_nation_pop_df [ \"pop_year\" ] = pop_year # Change all column names to str whole_nation_pop_df . columns = whole_nation_pop_df . columns . astype ( str ) # Write df out to feather for quicker reading print ( \"Writing whole_nation_pop_df.feather\" ) whole_nation_pop_df . reset_index () . to_feather ( national_pop_feather_path ) else : # if it exists, read from a feather for quicker data retreval print ( f \"Reading whole_nation_pop_df from { national_pop_feather_path } \" ) whole_nation_pop_df = pd . read_feather ( national_pop_feather_path ) # Temporary TODO: remove this line whole_nation_pop_df . rename ( columns = { \"total_pop\" : \"pop_count\" }, inplace = True ) return whole_nation_pop_df read_ni_age_df ( path ) Reads in the nothern ireland age information Parameters: path ( str ) \u2013 the path of the file Returns: \u2013 pd.DataFrame the age_df dataframe src/data_ingest.py 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 def read_ni_age_df ( path ): \"\"\" Reads in the nothern ireland age information Args: path (str): the path of the file Returns: pd.DataFrame the age_df dataframe \"\"\" # read in age df age_df = pd . read_excel ( path , sheet_name = \"SA\" , header = 5 , index_col = \"SA Code\" ) # replace col names from string e.g \"Age 82\" # to just the number 82 for col in age_df . columns : if bool ( re . search ( r \"\\d\" , col )): number = re . findall ( r \"[0-9]{1,3}\" , col )[ 0 ] age_df . rename ( columns = { col : number }, inplace = True ) return age_df read_scottish_age ( path ) Reads in the scottish age information Parameters: path ( str ) \u2013 the path of the file Returns: Bool \u2013 True if path has not been modified within the number of days specified src/data_ingest.py 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 def read_scottish_age ( path ): \"\"\" Reads in the scottish age information Args: path (str): the path of the file Returns: Bool: True if path has not been modified within the number of days specified \"\"\" # read in scottish file age_scotland_df = pd . read_csv ( path , skiprows = 4 ) # dropping first row as this is the whole of scotland age_scotland_df = age_scotland_df . iloc [ 1 : - 4 , :] age_scotland_df = age_scotland_df . reset_index () return age_scotland_df read_urb_rur_class_scotland ( urb_rur_path ) Reads the urb/rural classification for Scotland. This reads in the file containing all the urban/rural class Then applies a mapping based on if living in a settlement >10,000 then urban, else rural. Parameters: path ( str ) \u2013 the path where the file exists Returns: \u2013 pd.DataFrame: the classfication dataframe src/data_ingest.py 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 def read_urb_rur_class_scotland ( urb_rur_path ): \"\"\"Reads the urb/rural classification for Scotland. This reads in the file containing all the urban/rural class Then applies a mapping based on if living in a settlement >10,000 then urban, else rural. Args: path (str): the path where the file exists Returns: pd.DataFrame: the classfication dataframe \"\"\" urb_rur = pd . read_csv ( urb_rur_path , usecols = [ \"OA2011\" , \"UR6_2013_2014\" ]) urb_rur [ \"urb_rur_class\" ] = np . where ( ( urb_rur [ \"UR6_2013_2014\" ] == 1 ) | ( urb_rur [ \"UR6_2013_2014\" ] == 2 ), \"urban\" , \"rural\" ) return urb_rur read_urb_rur_ni ( urb_rur_path ) Reads small areas as urban or rural for Northern Ireland. Urban is classified as living in a settlement >10,000, and rural is classified as <10,000. Parameters: path ( str ) \u2013 the path where the file exists Returns: \u2013 pd.DataFrame: the urban rural classification dataframe src/data_ingest.py 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 def read_urb_rur_ni ( urb_rur_path ): \"\"\" Reads small areas as urban or rural for Northern Ireland. Urban is classified as living in a settlement >10,000, and rural is classified as <10,000. Args: path (str): the path where the file exists Returns: pd.DataFrame: the urban rural classification dataframe \"\"\" urb_rur = pd . read_csv ( urb_rur_path , skiprows = 3 ) urb_rur = urb_rur [[ 'SA2011_Code' , 'Settlement Classification Band' ]] # split classification bands into urban and rural urb_list = [ 'A' , 'B' , 'C' , 'D' ] rural_list = [ 'E' , 'F' , 'G' , 'H' ] urb_rur . loc [ urb_rur [ 'Settlement Classification Band' ] . isin ( urb_list ), 'urb_rur_class' ] = 'urban' urb_rur . loc [ urb_rur [ 'Settlement Classification Band' ] . isin ( rural_list ), 'urb_rur_class' ] = 'rural' return urb_rur read_usual_pop_scotland ( path ) Reads the usual population Scotland. This reads in the file containing all the output areas in scotland with their corresponding population number. Parameters: path ( str ) \u2013 the path where the file exists Returns: \u2013 pd.DataFrame the usual population dataframe src/data_ingest.py 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 def read_usual_pop_scotland ( path : str ): \"\"\"Reads the usual population Scotland. This reads in the file containing all the output areas in scotland with their corresponding population number. Args: path (str): the path where the file exists Returns: pd.DataFrame the usual population dataframe \"\"\" # reads in data and crops of header and footer df = pd . read_csv ( path , header = 4 , index_col = 0 , skipfooter = 4 ) # only want OA so drop Scotland df_oa_only = df . drop ( index = [ 'Scotland' ]) # only use columns that we need essential_cols = [ \"All people\" , \"Males\" , \"Females\" ] df_essential_cols = df_oa_only [ essential_cols ] # ensure no commas in the dataset so no errors with dtypes for col in essential_cols : df_essential_cols [ col ] = df_essential_cols [ col ] . str . replace ( ',' , '' ) df_essential_cols [ col ] = df_essential_cols [ col ] . astype ( int ) return df_essential_cols save_latest_stops_as_feather ( file_name ) Saves the latest stop file as a feather file into the data folder. Parameters: file_name ( str ) \u2013 file path for latest stop file. src/data_ingest.py 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def save_latest_stops_as_feather ( file_name ): \"\"\"Saves the latest stop file as a feather file into the data folder. Args: file_name (str): file path for latest stop file. \"\"\" # read in csv file = pd . read_csv ( file_name , usecols = config [ \"naptan_types\" ] . keys (), dtype = config [ \"naptan_types\" ]) # get output path output_path = os . path . join ( os . getcwd (), \"data\" , \"stops\" , \"Stops.feather\" ) # output to feather file . to_feather ( output_path ) return output_path","title":"Data ingest"},{"location":"data_ingest/#src.data_ingest.any_to_pd","text":"A function which ties together many other data ingest related functions to import data. Currently this function can handle the remote or local import of data from zip (containing csv files), and csv files. The main purpose is to check for locally stored persistent data files and get that data into a dataframe for further processing. Each time the function checks for download/extracted data so it doesn't have to go through the process again. Firstly the function checks for a feather file and loads that if available. If the feather is not available the function checks for a csv file, then loads that if available. If no csv file is available it checks for a zip file, from which to extract the csv from. If a zip file is not available locally it falls back to downloading the zip file (which could contains other un-needed datasets) then extracts the specified/needed data set (csv) and deletes the now un-needed zip file. This function should be used in place of pd.read_csv for example. Parameters: file_nm ( str ) \u2013 The name of the file without extension. zip_link ( str ) \u2013 URL containing zipped data. ext_order ( list ) \u2013 The order in which to try extraction methods. dtypes ( Optional [ Dict ] ) \u2013 Datatypes of columns in the csv. Returns: pd . DataFrame \u2013 pd.DataFrame: A dataframe of the data that has been imported. src/data_ingest.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def any_to_pd ( file_nm : str , zip_link : str , ext_order : List , dtypes : Optional [ Dict ], data_dir = DATA_DIR ) -> pd . DataFrame : \"\"\"A function which ties together many other data ingest related functions to import data. Currently this function can handle the remote or local import of data from zip (containing csv files), and csv files. The main purpose is to check for locally stored persistent data files and get that data into a dataframe for further processing. Each time the function checks for download/extracted data so it doesn't have to go through the process again. Firstly the function checks for a feather file and loads that if available. If the feather is not available the function checks for a csv file, then loads that if available. If no csv file is available it checks for a zip file, from which to extract the csv from. If a zip file is not available locally it falls back to downloading the zip file (which could contains other un-needed datasets) then extracts the specified/needed data set (csv) and deletes the now un-needed zip file. This function should be used in place of pd.read_csv for example. Parameters: file_nm (str): The name of the file without extension. zip_link (str): URL containing zipped data. ext_order (list): The order in which to try extraction methods. dtypes (Optional[Dict]): Datatypes of columns in the csv. Returns: pd.DataFrame: A dataframe of the data that has been imported. \"\"\" # Change directory into project root os . chdir ( CWD ) # Make the load order (lists are ordered) to prioritise load_order = [ f \" { file_nm } . { ext } \" for ext in ext_order ] # make a list of functions that apply to these files load_funcs = { \"feather\" : _feath_to_df , \"csv\" : _csv_to_df , \"zip\" : _import_extract_delete_zip } # create a dictionary ready to dispatch functions # load_func_dict = {f\"{file_name}\": load_func # Iterate through files that might exist for i in range ( len ( load_order )): # Indexing with i because the list loading in the wrong order data_file_nm = load_order [ i ] data_file_path = _make_data_path ( data_dir , data_file_nm ) if _persistent_exists ( data_file_path ): # Check if each persistent file exists # load the persistent file by dispatching the correct function if dtypes and ext_order [ i ] == \"csv\" : pd_df = load_funcs [ ext_order [ i ]]( file_nm , data_file_path , dtypes = dtypes ) else : pd_df = load_funcs [ ext_order [ i ]]( file_nm , data_file_path ) return pd_df continue # None of the persistent files has been found. # Continue onto the next file type # A zip must be downloaded, extracted, and turned into pd_df pd_df = load_funcs [ ext_order [ i ]]( file_nm , data_file_path , persistent_exists = False , zip_url = zip_link , dtypes = dtypes ) return pd_df","title":"any_to_pd()"},{"location":"data_ingest/#src.data_ingest.best_before","text":"Checks whether a path has been modified within a period of days. Parameters: path ( str ) \u2013 the path to check number_of_days ( int ) \u2013 number of days previous to check for modifications Returns: Bool \u2013 True if path has not been modified within the number of days specified src/data_ingest.py 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 def best_before ( path , number_of_days ): \"\"\" Checks whether a path has been modified within a period of days. Args: path (str): the path to check number_of_days (int): number of days previous to check for modifications Returns: Bool: True if path has not been modified within the number of days specified \"\"\" todays_date = datetime . today () last_modified_date = datetime . fromtimestamp ( os . path . getmtime ( path )) days_since_last_modification = ( todays_date - last_modified_date ) . days if days_since_last_modification > number_of_days : expired = True else : expired = False return expired","title":"best_before()"},{"location":"data_ingest/#src.data_ingest.capture_region","text":"Extracts the region name from the ONS population estimate excel files. Parameters: file_nm ( str ) \u2013 Full name of the regional population Excel file. Returns: str \u2013 The name of the region that the file covers src/data_ingest.py 386 387 388 389 390 391 392 393 394 395 396 397 398 def capture_region ( file_nm : str ): \"\"\"Extracts the region name from the ONS population estimate excel files. Args: file_nm (str): Full name of the regional population Excel file. Returns: str: The name of the region that the file covers \"\"\" patt = re . compile ( \"^(.*estimates[-]?)(?P<region>.*)(\\.xls)\" ) region = re . search ( patt , file_nm ) . group ( \"region\" ) region = region . replace ( \"-\" , \" \" ) . capitalize () return region","title":"capture_region()"},{"location":"data_ingest/#src.data_ingest.geo_df_from_geospatialfile","text":"Function to create a Geo-dataframe from a geospatial (geojson, shp) file. The process goes via Pandas.s Parameters: path_to_file ( str ) \u2013 path to the geojson, shp and other geospatial data files. Returns: \u2013 Geopandas Dataframe src/data_ingest.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 def geo_df_from_geospatialfile ( path_to_file , crs = 'epsg:27700' ): \"\"\"Function to create a Geo-dataframe from a geospatial (geojson, shp) file. The process goes via Pandas.s Args: path_to_file (str): path to the geojson, shp and other geospatial data files. Returns: Geopandas Dataframe \"\"\" geo_df = gpd . read_file ( path_to_file ) if geo_df . crs is None : geo_df . crs = 'epsg:27700' elif geo_df . crs != crs : geo_df = geo_df . to_crs ( 'epsg:27700' ) return geo_df","title":"geo_df_from_geospatialfile()"},{"location":"data_ingest/#src.data_ingest.geo_df_from_pd_df","text":"Function to create a Geo-dataframe from a Pandas DataFrame. Parameters: pd_df ( pd . DataFrame ) \u2013 a pandas dataframe object to be converted. geom_x ( str ) \u2013 name of the column that contains the longitude data. geom_y ( str ) \u2013 name of the column that contains the latitude data. crs ( str ) \u2013 the coordinate reference system required. Returns: \u2013 Geopandas Dataframe src/data_ingest.py 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def geo_df_from_pd_df ( pd_df , geom_x , geom_y , crs ): \"\"\"Function to create a Geo-dataframe from a Pandas DataFrame. Arguments: pd_df (pd.DataFrame): a pandas dataframe object to be converted. geom_x (str):name of the column that contains the longitude data. geom_y (str):name of the column that contains the latitude data. crs (str): the coordinate reference system required. Returns: Geopandas Dataframe \"\"\" geometry = [ Point ( xy ) for xy in zip ( pd_df [ geom_x ], pd_df [ geom_y ])] geo_df = gpd . GeoDataFrame ( pd_df , geometry = geometry ) geo_df . crs = crs geo_df . to_crs ( 'EPSG:27700' , inplace = True ) return geo_df","title":"geo_df_from_pd_df()"},{"location":"data_ingest/#src.data_ingest.get_and_save_geo_dataset","text":"Fetches a geodataset in json format from a web resource and saves it to the local data/ directory and returns the json as a dict into memory. Parameters: filename ( str ) \u2013 the name of file as it should be saved locally. url ( str ) \u2013 URL of the web resource where json file is hosted. localpath ( str ) \u2013 path to folder where json is to be saved locally. Returns: \u2013 dict src/data_ingest.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 def get_and_save_geo_dataset ( url , localpath , filename ): \"\"\"Fetches a geodataset in json format from a web resource and saves it to the local data/ directory and returns the json as a dict into memory. Args: filename (str): the name of file as it should be saved locally. url (str): URL of the web resource where json file is hosted. localpath (str): path to folder where json is to be saved locally. Returns: dict \"\"\" file = requests . get ( url ) . json () full_path = os . path . join ( localpath , filename ) with open ( full_path , 'w' ) as dset : json . dump ( file , dset ) return file","title":"get_and_save_geo_dataset()"},{"location":"data_ingest/#src.data_ingest.get_oa_la_csv_abspath","text":"Takes a directory as str and returns the absolute path of output area csv file. Parameters: dir ( str ) \u2013 Path created with os.path.join. Returns: str \u2013 Absolute path of the csv file of the Output area. src/data_ingest.py 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 def get_oa_la_csv_abspath ( dir ): \"\"\"Takes a directory as str and returns the absolute path of output area csv file. Args: dir (str): Path created with os.path.join. Returns: str: Absolute path of the csv file of the Output area. \"\"\" # Add check that the directory exists and is not empty if not os . path . exists ( dir ) or len ( os . listdir ( dir )) == 0 : raise ValueError ( f \"Directory { dir } does not exist or is empty\" ) files = os . listdir ( dir ) csv_files = [ file for file in files if file . endswith ( \".csv\" )] csv_file = csv_files [ 0 ] absolute_path = os . path . join ( dir , csv_file ) return absolute_path","title":"get_oa_la_csv_abspath()"},{"location":"data_ingest/#src.data_ingest.get_shp_abs_path","text":"Passed a directory into the function and returns the absolute path of where the shp file is within that directory. Parameters: dir ( PathLike ) \u2013 /path/to/directory/of/shape_files. Returns: str \u2013 the absolute path of the .shp file within a directory. src/data_ingest.py 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 def get_shp_abs_path ( dir ): \"\"\"Passed a directory into the function and returns the absolute path of where the shp file is within that directory. Args: dir (PathLike): /path/to/directory/of/shape_files. Returns: str: the absolute path of the .shp file within a directory. \"\"\" files = os . listdir ( dir ) shp_files = [ file for file in files if file . endswith ( \".shp\" )] # Add warning if there isn't a shp file in the directory if len ( shp_files ) == 0 : raise ValueError ( f \"No .shp file in directory { dir } \" ) shp_file = shp_files [ 0 ] absolute_path = os . path . join ( dir , shp_file ) return absolute_path","title":"get_shp_abs_path()"},{"location":"data_ingest/#src.data_ingest.get_stops_file","text":"Gets the latest stop dataset. If the latest stop df from the api is older then 28 days then function grabs a new version of file from API and saves this as a feather file. If the latest stop df from the api is less then 28 days old then just grabs the feather file. Parameters: url ( str ) \u2013 NAPTAN API url. dir ( str ) \u2013 directory where the stop data is stored. Returns: \u2013 pd.DataFrame src/data_ingest.py 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 def get_stops_file ( url , dir ): \"\"\"Gets the latest stop dataset. If the latest stop df from the api is older then 28 days then function grabs a new version of file from API and saves this as a feather file. If the latest stop df from the api is less then 28 days old then just grabs the feather file. Args: url (str): NAPTAN API url. dir (str): directory where the stop data is stored. Returns: pd.DataFrame \"\"\" # gets todays date and latest date of stops df today = int ( datetime . now () . strftime ( '%Y%m %d ' )) # gets feather stop path feather_path = os . path . join ( os . getcwd (), \"data\" , \"stops\" , \"Stops.feather\" ) # Check that the feather exists if not _persistent_exists ( feather_path ): stops_df = _dl_stops_make_df ( today , url ) else : # does exist latest_date = _get_latest_stop_file_date ( dir ) if today - latest_date < 28 : stops_df = pd . read_feather ( feather_path ) else : stops_df = _dl_stops_make_df ( today , url ) return stops_df","title":"get_stops_file()"},{"location":"data_ingest/#src.data_ingest.get_whole_nation_pop_df","text":"Gets the population data for all regions in the country and puts them into one dataframe. Parameters: pop_files ( list ) \u2013 Population data to be unioned. pop_year ( str ) \u2013 The year of population estimation data to process. Returns: \u2013 pd.DataFrame: Dataframe of population data for all regions in the country src/data_ingest.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 def get_whole_nation_pop_df ( pop_files , pop_year ): \"\"\"Gets the population data for all regions in the country and puts them into one dataframe. Args: pop_files (list): Population data to be unioned. pop_year (str): The year of population estimation data to process. Returns: pd.DataFrame: Dataframe of population data for all regions in the country \"\"\" # Remove gitkeep file from list of pop files pop_files = [ f for f in pop_files if f != '.gitkeep' ] # Dict of region:file_name. Capture the region name from the filename region_dict = { capture_region ( file ): file for file in pop_files } # make a df of each region then concat national_pop_feather_path = os . path . join ( DATA_DIR , f \"whole_nation_ { pop_year } .feather\" ) if not os . path . exists ( national_pop_feather_path ): print ( f \"No national_pop_feather found for { pop_year } \" ) print ( f \"Rebuilding population file. This will take a while!\" ) region_dfs_dict = {} for region in region_dict : print ( f \"Reading { region } Excel file\" ) xls_path = os . path . join ( DATA_DIR , \"population_estimates\" , str ( pop_year ), region_dict [ region ]) # Read Excel file as object xlFile = pd . ExcelFile ( xls_path ) # Access sheets in Excel file total_pop = pd . read_excel ( xlFile , f \"Mid- { pop_year } Persons\" , header = 4 ) males_pop = pd . read_excel ( xlFile , f \"Mid- { pop_year } Males\" , header = 4 , usecols = [ \"OA11CD\" , \"LSOA11CD\" , \"All Ages\" ]) fem_pop = pd . read_excel ( xlFile , f \"Mid- { pop_year } Females\" , header = 4 , usecols = [ \"OA11CD\" , \"LSOA11CD\" , \"All Ages\" ]) # Rename the \"All Ages\" columns appropriately before concating total_pop . rename ( columns = { \"All Ages\" : \"pop_count\" }, inplace = True ) males_pop . rename ( columns = { \"All Ages\" : \"males_pop\" }, inplace = True ) fem_pop . rename ( columns = { \"All Ages\" : \"fem_pop\" }, inplace = True ) # Merge the data from different sheets dfs_to_merge = [ total_pop , males_pop , fem_pop ] df_final = reduce ( lambda left , right : pd . merge ( left , right , on = 'OA11CD' ), dfs_to_merge ) # Store merged df in dict under region name region_dfs_dict [ region ] = df_final # Concat all regions into national pop df whole_nation_pop_df = pd . concat ( region_dfs_dict . values ()) # Create the pop_year column to show which year the data is from whole_nation_pop_df [ \"pop_year\" ] = pop_year # Change all column names to str whole_nation_pop_df . columns = whole_nation_pop_df . columns . astype ( str ) # Write df out to feather for quicker reading print ( \"Writing whole_nation_pop_df.feather\" ) whole_nation_pop_df . reset_index () . to_feather ( national_pop_feather_path ) else : # if it exists, read from a feather for quicker data retreval print ( f \"Reading whole_nation_pop_df from { national_pop_feather_path } \" ) whole_nation_pop_df = pd . read_feather ( national_pop_feather_path ) # Temporary TODO: remove this line whole_nation_pop_df . rename ( columns = { \"total_pop\" : \"pop_count\" }, inplace = True ) return whole_nation_pop_df","title":"get_whole_nation_pop_df()"},{"location":"data_ingest/#src.data_ingest.read_ni_age_df","text":"Reads in the nothern ireland age information Parameters: path ( str ) \u2013 the path of the file Returns: \u2013 pd.DataFrame the age_df dataframe src/data_ingest.py 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 def read_ni_age_df ( path ): \"\"\" Reads in the nothern ireland age information Args: path (str): the path of the file Returns: pd.DataFrame the age_df dataframe \"\"\" # read in age df age_df = pd . read_excel ( path , sheet_name = \"SA\" , header = 5 , index_col = \"SA Code\" ) # replace col names from string e.g \"Age 82\" # to just the number 82 for col in age_df . columns : if bool ( re . search ( r \"\\d\" , col )): number = re . findall ( r \"[0-9]{1,3}\" , col )[ 0 ] age_df . rename ( columns = { col : number }, inplace = True ) return age_df","title":"read_ni_age_df()"},{"location":"data_ingest/#src.data_ingest.read_scottish_age","text":"Reads in the scottish age information Parameters: path ( str ) \u2013 the path of the file Returns: Bool \u2013 True if path has not been modified within the number of days specified src/data_ingest.py 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 def read_scottish_age ( path ): \"\"\" Reads in the scottish age information Args: path (str): the path of the file Returns: Bool: True if path has not been modified within the number of days specified \"\"\" # read in scottish file age_scotland_df = pd . read_csv ( path , skiprows = 4 ) # dropping first row as this is the whole of scotland age_scotland_df = age_scotland_df . iloc [ 1 : - 4 , :] age_scotland_df = age_scotland_df . reset_index () return age_scotland_df","title":"read_scottish_age()"},{"location":"data_ingest/#src.data_ingest.read_urb_rur_class_scotland","text":"Reads the urb/rural classification for Scotland. This reads in the file containing all the urban/rural class Then applies a mapping based on if living in a settlement >10,000 then urban, else rural. Parameters: path ( str ) \u2013 the path where the file exists Returns: \u2013 pd.DataFrame: the classfication dataframe src/data_ingest.py 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 def read_urb_rur_class_scotland ( urb_rur_path ): \"\"\"Reads the urb/rural classification for Scotland. This reads in the file containing all the urban/rural class Then applies a mapping based on if living in a settlement >10,000 then urban, else rural. Args: path (str): the path where the file exists Returns: pd.DataFrame: the classfication dataframe \"\"\" urb_rur = pd . read_csv ( urb_rur_path , usecols = [ \"OA2011\" , \"UR6_2013_2014\" ]) urb_rur [ \"urb_rur_class\" ] = np . where ( ( urb_rur [ \"UR6_2013_2014\" ] == 1 ) | ( urb_rur [ \"UR6_2013_2014\" ] == 2 ), \"urban\" , \"rural\" ) return urb_rur","title":"read_urb_rur_class_scotland()"},{"location":"data_ingest/#src.data_ingest.read_urb_rur_ni","text":"Reads small areas as urban or rural for Northern Ireland. Urban is classified as living in a settlement >10,000, and rural is classified as <10,000. Parameters: path ( str ) \u2013 the path where the file exists Returns: \u2013 pd.DataFrame: the urban rural classification dataframe src/data_ingest.py 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 def read_urb_rur_ni ( urb_rur_path ): \"\"\" Reads small areas as urban or rural for Northern Ireland. Urban is classified as living in a settlement >10,000, and rural is classified as <10,000. Args: path (str): the path where the file exists Returns: pd.DataFrame: the urban rural classification dataframe \"\"\" urb_rur = pd . read_csv ( urb_rur_path , skiprows = 3 ) urb_rur = urb_rur [[ 'SA2011_Code' , 'Settlement Classification Band' ]] # split classification bands into urban and rural urb_list = [ 'A' , 'B' , 'C' , 'D' ] rural_list = [ 'E' , 'F' , 'G' , 'H' ] urb_rur . loc [ urb_rur [ 'Settlement Classification Band' ] . isin ( urb_list ), 'urb_rur_class' ] = 'urban' urb_rur . loc [ urb_rur [ 'Settlement Classification Band' ] . isin ( rural_list ), 'urb_rur_class' ] = 'rural' return urb_rur","title":"read_urb_rur_ni()"},{"location":"data_ingest/#src.data_ingest.read_usual_pop_scotland","text":"Reads the usual population Scotland. This reads in the file containing all the output areas in scotland with their corresponding population number. Parameters: path ( str ) \u2013 the path where the file exists Returns: \u2013 pd.DataFrame the usual population dataframe src/data_ingest.py 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 def read_usual_pop_scotland ( path : str ): \"\"\"Reads the usual population Scotland. This reads in the file containing all the output areas in scotland with their corresponding population number. Args: path (str): the path where the file exists Returns: pd.DataFrame the usual population dataframe \"\"\" # reads in data and crops of header and footer df = pd . read_csv ( path , header = 4 , index_col = 0 , skipfooter = 4 ) # only want OA so drop Scotland df_oa_only = df . drop ( index = [ 'Scotland' ]) # only use columns that we need essential_cols = [ \"All people\" , \"Males\" , \"Females\" ] df_essential_cols = df_oa_only [ essential_cols ] # ensure no commas in the dataset so no errors with dtypes for col in essential_cols : df_essential_cols [ col ] = df_essential_cols [ col ] . str . replace ( ',' , '' ) df_essential_cols [ col ] = df_essential_cols [ col ] . astype ( int ) return df_essential_cols","title":"read_usual_pop_scotland()"},{"location":"data_ingest/#src.data_ingest.save_latest_stops_as_feather","text":"Saves the latest stop file as a feather file into the data folder. Parameters: file_name ( str ) \u2013 file path for latest stop file. src/data_ingest.py 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def save_latest_stops_as_feather ( file_name ): \"\"\"Saves the latest stop file as a feather file into the data folder. Args: file_name (str): file path for latest stop file. \"\"\" # read in csv file = pd . read_csv ( file_name , usecols = config [ \"naptan_types\" ] . keys (), dtype = config [ \"naptan_types\" ]) # get output path output_path = os . path . join ( os . getcwd (), \"data\" , \"stops\" , \"Stops.feather\" ) # output to feather file . to_feather ( output_path ) return output_path","title":"save_latest_stops_as_feather()"},{"location":"data_output/","text":"reorder_final_df ( df ) Reorders the processed dataframe before writing to csv. Parameters: df ( pd . DataFrame ) \u2013 Dataframe to reorder. Returns: \u2013 pd.DataFrame: Reordered dataframe. src/data_output.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def reorder_final_df ( df ): \"\"\"Reorders the processed dataframe before writing to csv. Args: df (pd.DataFrame): Dataframe to reorder. Returns: pd.DataFrame: Reordered dataframe. \"\"\" df = df [[ \"Year\" , \"Sex\" , \"Age\" , \"Disability Status\" , \"Local Authority\" , \"Urban/Rural\" , \"Series\" , \"Observation Status\" , \"Unit Multiplier\" , \"Unit Measure\" , \"Value\" ]] return df reshape_for_output ( df , id_col , local_auth , id_rename = None ) Reshapes the output of served_proportions_disagg to data team requirements. The steps the function goes through are as follows. | 1) Transpose the df | 2) Reset the index | 3) Rename column from index to Age or other id column | 4) melt df with Age as ID vars, all the rest value vars | 5) Replace word \"Total\" with blanks, \"variable\" with \"Series\" | 6) Create \"Unit Multiplier\" map across from variable (percent or individual) | 7) Create \"Unit Measure\" and \"Observation Status\" columns | 7) Create the local auth col with this iteration's LA | 8) Rename column header of ID var | 9) re-order columns to match required output Parameters: df ( pd . DataFrame ) \u2013 Dataframe to reshape. id_col ( str ) \u2013 Name of the column that index will get renamed to. local_auth ( str ) \u2013 The local authority of interest. id_rename ( str ) \u2013 Name if renaming ID column. Defaults to None. Returns: \u2013 pd.DataFrame: Reshaped dataframe. src/data_output.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def reshape_for_output ( df , id_col , local_auth , id_rename = None ): \"\"\" Reshapes the output of served_proportions_disagg to data team requirements. The steps the function goes through are as follows. | 1) Transpose the df | 2) Reset the index | 3) Rename column from index to Age or other id column | 4) melt df with Age as ID vars, all the rest value vars | 5) Replace word \"Total\" with blanks, \"variable\" with \"Series\" | 6) Create \"Unit Multiplier\" map across from variable (percent or individual) | 7) Create \"Unit Measure\" and \"Observation Status\" columns | 7) Create the local auth col with this iteration's LA | 8) Rename column header of ID var | 9) re-order columns to match required output Args: df (pd.DataFrame): Dataframe to reshape. id_col (str): Name of the column that index will get renamed to. local_auth (str): The local authority of interest. id_rename (str, optional): Name if renaming ID column. Defaults to None. Returns: pd.DataFrame: Reshaped dataframe. \"\"\" # Transpose the df df = df . T # Reset the index df = df . reset_index () # Rename column from index to id_column, e.g \"Age\" df_renamed = df . rename ( columns = { \"index\" : id_col }) # Melt df with Age as ID vars, all the rest value vars df = pd . melt ( df_renamed , id_vars = id_col , value_vars = [ \"Total\" , \"Served\" , \"Unserved\" , \"Percentage served\" , \"Percentage unserved\" ]) # add in total population for la when id_col = total if id_col == \"Total\" : extra_row = pd . DataFrame ({ \"Total\" : [ \"Total\" ], \"variable\" : [ \"\" ], \"value\" : [ df_renamed [ \"All_pop\" ][ 0 ]]}) df = pd . concat ([ extra_row , df ]) # Replace word \"Total\" with blanks df = df . replace ({ \"Total\" : \"\" }) # Create \"Unit Multiplier\" map across from variable (percent or individual) df [ \"Unit Measure\" ] = np . where ( df . variable . str . contains ( \"Percentage\" ), \"percent\" , \"individual\" ) # Create \"Unit Measure\" and \"Observation Status\" columns df [ \"Unit Multiplier\" ] = \"Units\" df [ \"Observation Status\" ] = \"Undefined\" # Rename the variables in the \"variable\" column df [ \"variable\" ] . replace ( to_replace = \"Percentage served\" , value = \"Served\" , inplace = True ) df [ \"variable\" ] . replace ( to_replace = \"Percentage unserved\" , value = \"Unserved\" , inplace = True ) # Rename the \"variable\" col to \"Series\" df . rename ( columns = { \"variable\" : \"Series\" }, inplace = True ) # Rename \"value\" to \"Value\" as required df . rename ( columns = { \"value\" : \"Value\" }, inplace = True ) # Add and populate the \"Local Authority\" column df [ \"Local Authority\" ] = local_auth df = df [[ id_col , \"Local Authority\" , \"Series\" , \"Observation Status\" , \"Unit Multiplier\" , \"Unit Measure\" , \"Value\" ]] # Rename column header of ID var if id_rename : df . rename ( columns = { id_col : id_rename }, inplace = True ) return df","title":"Data output"},{"location":"data_output/#src.data_output.reorder_final_df","text":"Reorders the processed dataframe before writing to csv. Parameters: df ( pd . DataFrame ) \u2013 Dataframe to reorder. Returns: \u2013 pd.DataFrame: Reordered dataframe. src/data_output.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def reorder_final_df ( df ): \"\"\"Reorders the processed dataframe before writing to csv. Args: df (pd.DataFrame): Dataframe to reorder. Returns: pd.DataFrame: Reordered dataframe. \"\"\" df = df [[ \"Year\" , \"Sex\" , \"Age\" , \"Disability Status\" , \"Local Authority\" , \"Urban/Rural\" , \"Series\" , \"Observation Status\" , \"Unit Multiplier\" , \"Unit Measure\" , \"Value\" ]] return df","title":"reorder_final_df()"},{"location":"data_output/#src.data_output.reshape_for_output","text":"Reshapes the output of served_proportions_disagg to data team requirements. The steps the function goes through are as follows. | 1) Transpose the df | 2) Reset the index | 3) Rename column from index to Age or other id column | 4) melt df with Age as ID vars, all the rest value vars | 5) Replace word \"Total\" with blanks, \"variable\" with \"Series\" | 6) Create \"Unit Multiplier\" map across from variable (percent or individual) | 7) Create \"Unit Measure\" and \"Observation Status\" columns | 7) Create the local auth col with this iteration's LA | 8) Rename column header of ID var | 9) re-order columns to match required output Parameters: df ( pd . DataFrame ) \u2013 Dataframe to reshape. id_col ( str ) \u2013 Name of the column that index will get renamed to. local_auth ( str ) \u2013 The local authority of interest. id_rename ( str ) \u2013 Name if renaming ID column. Defaults to None. Returns: \u2013 pd.DataFrame: Reshaped dataframe. src/data_output.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def reshape_for_output ( df , id_col , local_auth , id_rename = None ): \"\"\" Reshapes the output of served_proportions_disagg to data team requirements. The steps the function goes through are as follows. | 1) Transpose the df | 2) Reset the index | 3) Rename column from index to Age or other id column | 4) melt df with Age as ID vars, all the rest value vars | 5) Replace word \"Total\" with blanks, \"variable\" with \"Series\" | 6) Create \"Unit Multiplier\" map across from variable (percent or individual) | 7) Create \"Unit Measure\" and \"Observation Status\" columns | 7) Create the local auth col with this iteration's LA | 8) Rename column header of ID var | 9) re-order columns to match required output Args: df (pd.DataFrame): Dataframe to reshape. id_col (str): Name of the column that index will get renamed to. local_auth (str): The local authority of interest. id_rename (str, optional): Name if renaming ID column. Defaults to None. Returns: pd.DataFrame: Reshaped dataframe. \"\"\" # Transpose the df df = df . T # Reset the index df = df . reset_index () # Rename column from index to id_column, e.g \"Age\" df_renamed = df . rename ( columns = { \"index\" : id_col }) # Melt df with Age as ID vars, all the rest value vars df = pd . melt ( df_renamed , id_vars = id_col , value_vars = [ \"Total\" , \"Served\" , \"Unserved\" , \"Percentage served\" , \"Percentage unserved\" ]) # add in total population for la when id_col = total if id_col == \"Total\" : extra_row = pd . DataFrame ({ \"Total\" : [ \"Total\" ], \"variable\" : [ \"\" ], \"value\" : [ df_renamed [ \"All_pop\" ][ 0 ]]}) df = pd . concat ([ extra_row , df ]) # Replace word \"Total\" with blanks df = df . replace ({ \"Total\" : \"\" }) # Create \"Unit Multiplier\" map across from variable (percent or individual) df [ \"Unit Measure\" ] = np . where ( df . variable . str . contains ( \"Percentage\" ), \"percent\" , \"individual\" ) # Create \"Unit Measure\" and \"Observation Status\" columns df [ \"Unit Multiplier\" ] = \"Units\" df [ \"Observation Status\" ] = \"Undefined\" # Rename the variables in the \"variable\" column df [ \"variable\" ] . replace ( to_replace = \"Percentage served\" , value = \"Served\" , inplace = True ) df [ \"variable\" ] . replace ( to_replace = \"Percentage unserved\" , value = \"Unserved\" , inplace = True ) # Rename the \"variable\" col to \"Series\" df . rename ( columns = { \"variable\" : \"Series\" }, inplace = True ) # Rename \"value\" to \"Value\" as required df . rename ( columns = { \"value\" : \"Value\" }, inplace = True ) # Add and populate the \"Local Authority\" column df [ \"Local Authority\" ] = local_auth df = df [[ id_col , \"Local Authority\" , \"Series\" , \"Observation Status\" , \"Unit Multiplier\" , \"Unit Measure\" , \"Value\" ]] # Rename column header of ID var if id_rename : df . rename ( columns = { id_col : id_rename }, inplace = True ) return df","title":"reshape_for_output()"},{"location":"data_quality/","text":"Data Quality NAPTAN The NaPTAN data is open source and available for anyone under a Government License. In our methodology we need to distinguish between high and low capacity transport. There is no specific marker for this in the dataset. Therefore we use the stop type variable to determine whether a stop is high or low capacity transport. Trams and Metro have the same stop_type. According to the UN definition, Metros are high capacity and Trams are low capacity. However we are unable to distinguish between the two in our data. Here is the issue in more detail. The data is submitted on an adhoc basis by local authorities. Details of their last submission can be seen here . As this is done on an adhoc basis there needs to be an element of downloading and timestamping data to ensure analysis is reproducible. Furthermore, the API which we get the data from is updated on a daily basis and it is difficult to tell whether the data has changed from the last upload. There is a column called status which determines whether a stop is inactive or active. We use this variable to ensure all stops used in our calculation are active. However, there are some other values such as pending and blank which are ambiguous. A conversation with the department of transport documented here clarified how to deal with each value. Even though it is the local authorities responsibility to submit to NaPTAN. It is unclear how they submit this information. Do they liaise with the bus companies to get this information and how timely is each submission? An important issue is that it is not clear whether each stop has step free access and is accessible to all. Therefore there could be stops that are in the data which can\u2019t be used. This is particularly relevant to our analysis as we produce breakdowns by disability. Population Weighted Centroids The OA Population Weighted Centroids are an official geographical product from the Office For National Statistics. These are used to determine where an Output Area can be allocated to, when aggregating statistics from OA to any other geographical level such as local authorities. Centroids are calculated via the median centroid algorithm. The methodology is detailed here . The population centroids are on an Output area basis. There are approximately 300 people living in an OA, therefore it automatically assumes that all 300 people live within this one point. As we apply a Eucliden buffer around the centroid (a circle with radius 500/1000M) there may not be an accessible path/pavement or road to get from the centroid to the stop that lies within the buffer. The centroid might be in the middle of a lake or field and therefore not actually representative of where the population actually live.The centroids were calculated in 2011. There is a possibility that the areas in which people lived in have changed. For example if a new housing estate has been built since 2011, this may shift where the centroid lies. Urban/Rural Classification The definition of whether an OA is urban/rural can be found here . This is one definition of how to classify this. The EU uses a different definition and therefore there would be differences in the output statistics. Urban/Rural classification uses a 2011 classification applied to the current OA boundaries. Classification of urban/rural might have changed since then. For example if a big housing estate was created in the countryside on the edge of a city, this might lead to that OA being classified as urban instead of rural. Disability Data The disability data used iis from the 2011 census, therefore a proportion is applied to population estimates for subsequent years. This method was suggested by the ONS Geo Spatial Department. These demographics might have changed since then. The definition used for disability is the GSS harmonized disability data . The UN definition appears to be much wider for disability. This could lead to different output statistics when aggregating up. Annual Population Estimates These are the annual population estimates that ONS produce every year. They are an official statistic. These, unlike the census, are estimates and have a defined methodology here . The methodology includes a section on statistical uncertainty about these numbers. LAD Boundaries There are several different types of geo spatial boundaries that can be used. (BFE) Best Full Extent is the boundary type we use. To be consistent with official statistics that ONS produce this is the type of boundary we use. LA Boundaries are subject to change year on year. Big changes can occur such as merging of local authorities and name changes. Therefore comparisons between years need to be made carefully. There is a discussion on this point documented here .","title":"Data Quality"},{"location":"data_quality/#data-quality","text":"","title":"Data Quality"},{"location":"data_quality/#naptan","text":"The NaPTAN data is open source and available for anyone under a Government License. In our methodology we need to distinguish between high and low capacity transport. There is no specific marker for this in the dataset. Therefore we use the stop type variable to determine whether a stop is high or low capacity transport. Trams and Metro have the same stop_type. According to the UN definition, Metros are high capacity and Trams are low capacity. However we are unable to distinguish between the two in our data. Here is the issue in more detail. The data is submitted on an adhoc basis by local authorities. Details of their last submission can be seen here . As this is done on an adhoc basis there needs to be an element of downloading and timestamping data to ensure analysis is reproducible. Furthermore, the API which we get the data from is updated on a daily basis and it is difficult to tell whether the data has changed from the last upload. There is a column called status which determines whether a stop is inactive or active. We use this variable to ensure all stops used in our calculation are active. However, there are some other values such as pending and blank which are ambiguous. A conversation with the department of transport documented here clarified how to deal with each value. Even though it is the local authorities responsibility to submit to NaPTAN. It is unclear how they submit this information. Do they liaise with the bus companies to get this information and how timely is each submission? An important issue is that it is not clear whether each stop has step free access and is accessible to all. Therefore there could be stops that are in the data which can\u2019t be used. This is particularly relevant to our analysis as we produce breakdowns by disability.","title":"NAPTAN"},{"location":"data_quality/#population-weighted-centroids","text":"The OA Population Weighted Centroids are an official geographical product from the Office For National Statistics. These are used to determine where an Output Area can be allocated to, when aggregating statistics from OA to any other geographical level such as local authorities. Centroids are calculated via the median centroid algorithm. The methodology is detailed here . The population centroids are on an Output area basis. There are approximately 300 people living in an OA, therefore it automatically assumes that all 300 people live within this one point. As we apply a Eucliden buffer around the centroid (a circle with radius 500/1000M) there may not be an accessible path/pavement or road to get from the centroid to the stop that lies within the buffer. The centroid might be in the middle of a lake or field and therefore not actually representative of where the population actually live.The centroids were calculated in 2011. There is a possibility that the areas in which people lived in have changed. For example if a new housing estate has been built since 2011, this may shift where the centroid lies.","title":"Population Weighted Centroids"},{"location":"data_quality/#urbanrural-classification","text":"The definition of whether an OA is urban/rural can be found here . This is one definition of how to classify this. The EU uses a different definition and therefore there would be differences in the output statistics. Urban/Rural classification uses a 2011 classification applied to the current OA boundaries. Classification of urban/rural might have changed since then. For example if a big housing estate was created in the countryside on the edge of a city, this might lead to that OA being classified as urban instead of rural.","title":"Urban/Rural Classification"},{"location":"data_quality/#disability-data","text":"The disability data used iis from the 2011 census, therefore a proportion is applied to population estimates for subsequent years. This method was suggested by the ONS Geo Spatial Department. These demographics might have changed since then. The definition used for disability is the GSS harmonized disability data . The UN definition appears to be much wider for disability. This could lead to different output statistics when aggregating up.","title":"Disability Data"},{"location":"data_quality/#annual-population-estimates","text":"These are the annual population estimates that ONS produce every year. They are an official statistic. These, unlike the census, are estimates and have a defined methodology here . The methodology includes a section on statistical uncertainty about these numbers.","title":"Annual Population Estimates"},{"location":"data_quality/#lad-boundaries","text":"There are several different types of geo spatial boundaries that can be used. (BFE) Best Full Extent is the boundary type we use. To be consistent with official statistics that ONS produce this is the type of boundary we use. LA Boundaries are subject to change year on year. Big changes can occur such as merging of local authorities and name changes. Therefore comparisons between years need to be made carefully. There is a discussion on this point documented here .","title":"LAD Boundaries"},{"location":"data_transform/","text":"bin_pop_ages ( age_df , age_bins , col_nms ) Bins the ages in the age_df in 5 year spans from 0 to 90+, sums the counts in those bins and drops the original age columns. Parameters: df ( pd . DataFrame ) \u2013 A dataframe of population data containing only the age columns. Returns: \u2013 pd.DataFrame: Returns the age_df with bins. src/data_transform.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def bin_pop_ages ( age_df , age_bins , col_nms ): \"\"\" Bins the ages in the age_df in 5 year spans from 0 to 90+, sums the counts in those bins and drops the original age columns. Args: df (pd.DataFrame): A dataframe of population data containing only the age columns. Returns: pd.DataFrame: Returns the age_df with bins. \"\"\" # Grouping ages in 5 year brackets # cleaning scottish data and changing dtype to float original_columns = age_df . columns for col in original_columns : if age_df [ col ] . dtypes == \"O\" : age_df [ col ] = age_df [ col ] . str . replace ( '-' , '0' ) age_df [ col ] = age_df [ col ] . astype ( int ) def _age_bin ( age_df , age_bins ): \"\"\"Function sums the counts for corresponding age-bins and assigns them a column in age_df.\"\"\" for bin in age_bins : age_df [ f \" { bin [ 0 ] } - { bin [ 1 ] } \" ] = ( age_df . loc [:, bin [ 0 ]: bin [ 1 ]] . sum ( axis = 1 )) return age_df # create 90+ column for when there are more columns than 90 if len ( age_df . columns ) > 91 : # create 90+ column summing all those from 90 and above. age_df [ '90+' ] = age_df . iloc [:, 90 :] . sum ( axis = 1 ) age_df = _age_bin ( age_df , age_bins ) # drop the original age columns age_df . drop ( col_nms , axis = 1 , inplace = True ) # drop the columns that we are replacing with 90+ age_df . drop ( age_df . iloc [:, 19 :], axis = 1 , inplace = True ) # moving first column to last so 90+ at the end. temp_cols = age_df . columns . tolist () new_cols = temp_cols [ 1 :] + temp_cols [ 0 : 1 ] age_df = age_df [ new_cols ] else : age_df = _age_bin ( age_df , age_bins ) # drop the original age columns age_df . drop ( col_nms , axis = 1 , inplace = True ) # rename the 90+ column age_df . rename ( columns = { '90+-90+' : '90+' }, inplace = True ) # age df has now been binned and cleaned return round ( age_df ) convert_east_north ( df , long , lat ) Converts latitude and longitude coordinates to British National Grid Parameters: df ( pd . DataFrame ) \u2013 df including the longitude and latitude coordinates long(str) \u2013 The name of the longitude column in df lat ( str ) \u2013 The name of the latitude column in df Returns: \u2013 pd.DataFrame: dataframe including easting and northing coordinates. src/data_transform.py 438 439 440 441 442 443 444 445 446 447 448 449 def convert_east_north ( df , long , lat ): \"\"\" Converts latitude and longitude coordinates to British National Grid Args: df (pd.DataFrame): df including the longitude and latitude coordinates long(str): The name of the longitude column in df lat (str): The name of the latitude column in df Returns: pd.DataFrame: dataframe including easting and northing coordinates. \"\"\" df [ 'Easting' ], df [ 'Northing' ] = convert_bng ( df [ long ], df [ lat ]) return df create_tiploc_col ( naptan_df ) Creates a Tiploc column from the ATCOCode column, in the NaPTAN dataset. Parameters: naptan_df ( pd . Dataframe ) \u2013 Naptan dataset Returns: \u2013 pd.Dataframe (naptan_df): Naptan dataset with the new tiploc column \u2013 added for train stations src/data_transform.py 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 def create_tiploc_col ( naptan_df ): \"\"\"Creates a Tiploc column from the ATCOCode column, in the NaPTAN dataset. Args: naptan_df (pd.Dataframe): Naptan dataset Returns: pd.Dataframe (naptan_df): Naptan dataset with the new tiploc column added for train stations \"\"\" # Applying only to train stations, RLY is the stop type for train stations rail_filter = naptan_df . StopType == \"RLY\" # Create a new pd.Dataframe for Tiploc by extracting upto 7 alpha # characters tiploc_col = ( naptan_df . loc [ rail_filter ] . ATCOCode . str . extract ( r '([A-Za-z]{1,7})' ) ) tiploc_col . columns = [ \"tiploc_code\" ] # Merge the new Tiploc column with the naptan_df naptan_df = naptan_df . merge ( tiploc_col , how = 'left' , left_index = True , right_index = True ) return naptan_df disab_dict ( la_pop_df , pop_in_poly_df , disability_dict , local_auth ) Creates the dataframe including those who are and are not served by public transport and places it into a disability dictionary for each local authority of interest for the final csv output. Parameters: la_pop_df ( gpd . GeoDataFrame ) \u2013 GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df ( gpd . GeoDataFrame ) \u2013 A geodata frame with the points inside the polygon. disability_dict ( dict ) \u2013 Dictionary to store the disability dataframe. local_auth ( str ) \u2013 The local authority of interest. Returns: disability_dict ( dict ) \u2013 Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. src/data_transform.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def disab_dict ( la_pop_df , pop_in_poly_df , disability_dict , local_auth ): \"\"\"Creates the dataframe including those who are and are not served by public transport and places it into a disability dictionary for each local authority of interest for the final csv output. Args: la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df (gpd.GeoDataFrame): A geodata frame with the points inside the polygon. disability_dict (dict): Dictionary to store the disability dataframe. local_auth (str): The local authority of interest. Returns: disability_dict (dict): Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. \"\"\" # Calculating those served and not served by disability disab_cols = [ \"number_disabled\" ] disab_servd_df = served_proportions_disagg ( la_pop_df , pop_in_poly_df , disab_cols ) # Feeding the results to the reshaper disab_servd_df_out = do . reshape_for_output ( disab_servd_df , id_col = disab_cols [ 0 ], local_auth = local_auth , id_rename = \"Disability Status\" ) # The disability df is unusual. I think all rows correspond to people with # disabilities only. There is no \"not-disabled\" status here (I think) disab_servd_df_out . replace ( to_replace = \"number_disabled\" , value = \"Disabled\" , inplace = True ) # Calculating non-disabled people served and not served non_disab_cols = [ \"number_non-disabled\" ] non_disab_servd_df = served_proportions_disagg ( pop_df = la_pop_df , pop_in_poly_df = pop_in_poly_df , cols_lst = non_disab_cols ) # Feeding the results to the reshaper non_disab_servd_df_out = do . reshape_for_output ( non_disab_servd_df , id_col = disab_cols [ 0 ], local_auth = local_auth , id_rename = \"Disability Status\" ) # The disability df is unusual. I think all rows correspond to people with # disabilities only. There is no \"not-disabled\" status here (I think) non_disab_servd_df_out . replace ( to_replace = \"number_non-disabled\" , value = \"Non-disabled\" , inplace = True ) # Concatting non-disabled and disabled dataframes non_disab_disab_servd_df_out = pd . concat ( [ non_disab_servd_df_out , disab_servd_df_out ]) # Output this local auth's disab df to the dict disability_dict [ local_auth ] = non_disab_disab_servd_df_out return disability_dict disab_disagg ( disability_df , la_pop_df ) Calculates number of people in the population that are classified as disabled or not disabled and this is merged onto the local authority population dataframe. Parameters: disability_df ( pd . DataFrame ) \u2013 Dataframe that includes disability estimates for each output area. la_pop_df ( gpd . GeoDataFrame ) \u2013 GeoPandas Dataframe that includes output area codes and population estimates. Returns: \u2013 gpd.GeoDataFrame: GeoPandas Dataframe that includes population estimates, geographical location, and proportion of disabled/non-disabled for each output area in the local authority chosen. src/data_transform.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def disab_disagg ( disability_df , la_pop_df ): \"\"\"Calculates number of people in the population that are classified as disabled or not disabled and this is merged onto the local authority population dataframe. Args: disability_df (pd.DataFrame): Dataframe that includes disability estimates for each output area. la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. Returns: gpd.GeoDataFrame: GeoPandas Dataframe that includes population estimates, geographical location, and proportion of disabled/non-disabled for each output area in the local authority chosen. \"\"\" # Getting the disab total disability_df [ \"disb_total\" ] = ( disability_df [ \"disab_ltd_lot\" ] + disability_df [ \"disab_ltd_little\" ]) # Calcualting the total \"non-disabled\" la_pop_only = la_pop_df [[ 'OA11CD' , 'pop_count' ]] disability_df = la_pop_only . merge ( disability_df , on = \"OA11CD\" ) # Putting the result back into the disability df disability_df [ \"non-disabled\" ] = disability_df [ \"pop_count\" ] - \\ disability_df [ 'disb_total' ] # Calculating the proportion of disabled people in each OA disability_df [ \"proportion_disabled\" ] = ( disability_df [ 'disb_total' ] / disability_df [ 'pop_count' ] ) # Calcualting the proportion of non-disabled people in each OA disability_df [ \"proportion_non-disabled\" ] = ( disability_df [ 'non-disabled' ] / disability_df [ 'pop_count' ] ) # Slice disability df that only has the proportion disabled column and the # OA11CD col disab_prop_df = disability_df [[ 'OA11CD' , 'proportion_disabled' , 'proportion_non-disabled' ]] # Merge the proportion disability df into main the pop df with a left join la_pop_df = la_pop_df . merge ( disab_prop_df , on = 'OA11CD' , how = \"left\" ) # Make the calculation of the number of people with disabilities in the # year of the population estimates la_pop_df [ \"number_disabled\" ] = ( round ( la_pop_df [ \"pop_count\" ] * la_pop_df [ \"proportion_disabled\" ]) ) # la_pop_df[\"number_disabled\"] = la_pop_df[\"number_disabled\"].astype(int) # Make the calculation of the number of non-disabled people in the year # of the population estimates la_pop_df [ \"number_non-disabled\" ] = ( round ( la_pop_df [ \"pop_count\" ] * la_pop_df [ \"proportion_non-disabled\" ]) ) la_pop_df [ \"number_non-disabled\" ] = la_pop_df [ \"number_non-disabled\" ] . astype ( int ) return la_pop_df get_col_bins ( col_nms ) Function to group/bin the ages together in 5 year steps. Starts the sequence at age 0. Will return the ages in a list of tuples. The 0th position of the tuple being the lower limit of the age bin, the 1st position of the tuple being the upper limit. Parameters: col_nms ( list of str ) \u2013 a list of the age columns as strings. Returns: \u2013 list of tuples: a list of the ages with 5 year gaps. src/data_transform.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_col_bins ( col_nms : List [ str ]): \"\"\"Function to group/bin the ages together in 5 year steps. Starts the sequence at age 0. Will return the ages in a list of tuples. The 0th position of the tuple being the lower limit of the age bin, the 1st position of the tuple being the upper limit. Args: col_nms (list of str): a list of the age columns as strings. Returns: list of tuples: a list of the ages with 5 year gaps. \"\"\" # Make a lists of starting and finishing indexes cols_start = col_nms [ 0 :: 5 ] cols_fin = col_nms [ 4 :: 5 ] # Generating a list of tuples which will be the age groupings col_bins = [( s , f ) for s , f in zip ( cols_start , cols_fin )] # Again adding \"90+\", doubling it so it's doubled, like the other tuples col_bins . append (( cols_start [ - 1 :] * 2 )) # TODO: make this more intelligent. Only if there is one col name left # over it should be doubled. return col_bins mid_year_age_estimates ( age_df , pop_estimates_df , pop_year ) Takes mid-year population estimates for each small area in NI and uses proportions to calculate mid-year estimates for each age. Parameters: age_df ( pd . DataFrame ) \u2013 Census 2011 age estimates dataframe pop_estimates_df ( pd . DataFrame ) \u2013 population estimates for each small area dataframe pop_year ( str ) \u2013 population year src/data_transform.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def mid_year_age_estimates ( age_df , pop_estimates_df , pop_year ): \"\"\" Takes mid-year population estimates for each small area in NI and uses proportions to calculate mid-year estimates for each age. Args: age_df (pd.DataFrame): Census 2011 age estimates dataframe pop_estimates_df (pd.DataFrame): population estimates for each small area dataframe pop_year (str): population year \"\"\" # get all age columns in a list age_cols = [ str ( y ) for y in range ( 101 )] # iterate through each age for age in range ( len ( age_cols )): # calculates the proportions for each age and each SA code age_df [ age_cols [ age ]] = age_df [ age_cols [ age ]] / age_df [ 'All usual residents' ] age_df . drop ([ 'SA' , 'All usual residents' ], axis = 1 , inplace = True ) # merges pop df and proportions together pop_estimates_df = pop_estimates_df . merge ( age_df . reset_index (), left_on = 'Area_Code' , right_on = 'SA Code' , how = 'left' ) # calculates pop estimates for each age in each small area using proportions for age in range ( len ( age_cols )): pop_estimates_df [ age_cols [ age ]] = pop_estimates_df [ age_cols [ age ]] * pop_estimates_df [ pop_year ] pop_estimates_df . set_index ( 'Area_Code' , inplace = True ) return pop_estimates_df served_proportions_disagg ( pop_df , pop_in_poly_df , cols_lst ) Calculates the number of people in each category, as specified by the column (e.g age range, or disability status) who are served and not served by public transport, and gives those as a proportion of the total. Note: the numeric values in the dataframe are return as strings for formatting reasons Parameters: pop_df ( pd . DataFrame ) \u2013 population dataframe. pop_in_poly_df ( pd . DataFrame ) \u2013 dataframe resulting in the points in polygons enquiry to count (sum) the population within the service area polygon. cols_lst ( List [ str ] ) \u2013 a list of the column names in the population dataframe supplied which contain population figures, and are to be summed and assessed as served/unserved by public transport. Returns: \u2013 pd.DataFrame: a dataframe summarising \u2013 i) the total number of people that column (e.g. age range, sex) \u2013 ii) the number served by public transport \u2013 iii) the proportion who are served by public transport \u2013 iv) the proportion who are not served by public transport src/data_transform.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def served_proportions_disagg ( pop_df : pd . DataFrame , pop_in_poly_df : pd . DataFrame , cols_lst : List [ str ]): \"\"\"Calculates the number of people in each category, as specified by the column (e.g age range, or disability status) who are served and not served by public transport, and gives those as a proportion of the total. Note: the numeric values in the dataframe are return as strings for formatting reasons Args: pop_df (pd.DataFrame): population dataframe. pop_in_poly_df (pd.DataFrame): dataframe resulting in the points in polygons enquiry to count (sum) the population within the service area polygon. cols_lst (List[str]): a list of the column names in the population dataframe supplied which contain population figures, and are to be summed and assessed as served/unserved by public transport. Returns: pd.DataFrame: a dataframe summarising i) the total number of people that column (e.g. age range, sex) ii) the number served by public transport iii) the proportion who are served by public transport iv) the proportion who are not served by public transport \"\"\" # First list the age bin columns pop_sums = {} for col in cols_lst : # Total pop total_pop = int ( pop_df [ col ] . sum ()) # Served pop servd_pop = int ( pop_in_poly_df [ col ] . sum ()) # Unserved pop unsrvd_pop = int ( total_pop - servd_pop ) if total_pop == 0 : # If the total population for that column is 0 # this standard of zeros and Nones is returned pop_sums [ col ] = { \"Total\" : str ( total_pop ), \"Served\" : str ( servd_pop ), \"Unserved\" : str ( unsrvd_pop ), \"Percentage served\" : \"None\" , \"Percentage unserved\" : \"None\" } elif total_pop > 0 : pop_sums [ col ] = _calc_proprtn_srvd_unsrvd ( total_pop , servd_pop , unsrvd_pop ) # Make a df from the total and served pop tot_servd_df = pd . DataFrame ( pop_sums ) return tot_servd_df slice_age_df ( df , col_nms ) Slices a dataframe according to the list of column names provided. Parameters: df ( pd . DataFrame ) \u2013 DataFrame to be sliced. col_nms ( List [ str ] ) \u2013 column names as string in a list. Returns: \u2013 pd.DataFrame: A dataframe sliced down to only the columns required. src/data_transform.py 26 27 28 29 30 31 32 33 34 35 36 37 def slice_age_df ( df : pd . DataFrame , col_nms : List [ str ]): \"\"\"Slices a dataframe according to the list of column names provided. Args: df (pd.DataFrame): DataFrame to be sliced. col_nms (List[str]): column names as string in a list. Returns: pd.DataFrame: A dataframe sliced down to only the columns required. \"\"\" age_df = df . loc [:, col_nms ] return age_df urban_rural_results ( la_pop_df , pop_in_poly_df , urb_rur_dict , local_auth ) Creates two dataframes, urban and rural and classifies proportion of people served and not served by public transport. This is placed into a urban rural dictionary for each local authority of interest for the final csv output. Parameters: la_pop_df ( gpd . GeoDataFrame ) \u2013 GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df ( gpd . GeoDataFrame ) \u2013 A geodata frame with the points inside the polygon. urb_rur_dict ( dict ) \u2013 Dictionary to store the urban rural dataframe. local_auth ( str ) \u2013 The local authority of interest. Returns: urb_rur_dict ( dict ) \u2013 Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. src/data_transform.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def urban_rural_results ( la_pop_df , pop_in_poly_df , urb_rur_dict , local_auth ): \"\"\" Creates two dataframes, urban and rural and classifies proportion of people served and not served by public transport. This is placed into a urban rural dictionary for each local authority of interest for the final csv output. Args: la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df (gpd.GeoDataFrame): A geodata frame with the points inside the polygon. urb_rur_dict (dict): Dictionary to store the urban rural dataframe. local_auth (str): The local authority of interest. Returns: urb_rur_dict (dict): Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. \"\"\" # Urban/Rural disaggregation # split into two different dataframes urb_df = la_pop_df [ la_pop_df . urb_rur_class == \"urban\" ] rur_df = la_pop_df [ la_pop_df . urb_rur_class == \"rural\" ] urb_df_poly = pop_in_poly_df [ pop_in_poly_df . urb_rur_class == \"urban\" ] rur_df_poly = pop_in_poly_df [ pop_in_poly_df . urb_rur_class == \"rural\" ] urb_servd_df = served_proportions_disagg ( pop_df = urb_df , pop_in_poly_df = urb_df_poly , cols_lst = [ 'pop_count' ]) rur_servd_df = served_proportions_disagg ( pop_df = rur_df , pop_in_poly_df = rur_df_poly , cols_lst = [ 'pop_count' ]) # Renaming pop_count to either urban or rural urb_servd_df . rename ( columns = { \"pop_count\" : \"Urban\" }, inplace = True ) rur_servd_df . rename ( columns = { \"pop_count\" : \"Rural\" }, inplace = True ) # Sending each to reshaper urb_servd_df_out = do . reshape_for_output ( urb_servd_df , id_col = \"Urban\" , local_auth = local_auth ) rur_servd_df_out = do . reshape_for_output ( rur_servd_df , id_col = \"Rural\" , local_auth = local_auth ) # Renaming their columns to Urban/Rural urb_servd_df_out . rename ( columns = { \"Urban\" : \"Urban/Rural\" }, inplace = True ) rur_servd_df_out . rename ( columns = { \"Rural\" : \"Urban/Rural\" }, inplace = True ) # Combining urban and rural dfs urb_rur_servd_df_out = pd . concat ([ urb_servd_df_out , rur_servd_df_out ]) # Output this iteration's urb and rur df to the dict urb_rur_dict [ local_auth ] = urb_rur_servd_df_out return urb_rur_dict","title":"Data transform"},{"location":"data_transform/#src.data_transform.bin_pop_ages","text":"Bins the ages in the age_df in 5 year spans from 0 to 90+, sums the counts in those bins and drops the original age columns. Parameters: df ( pd . DataFrame ) \u2013 A dataframe of population data containing only the age columns. Returns: \u2013 pd.DataFrame: Returns the age_df with bins. src/data_transform.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def bin_pop_ages ( age_df , age_bins , col_nms ): \"\"\" Bins the ages in the age_df in 5 year spans from 0 to 90+, sums the counts in those bins and drops the original age columns. Args: df (pd.DataFrame): A dataframe of population data containing only the age columns. Returns: pd.DataFrame: Returns the age_df with bins. \"\"\" # Grouping ages in 5 year brackets # cleaning scottish data and changing dtype to float original_columns = age_df . columns for col in original_columns : if age_df [ col ] . dtypes == \"O\" : age_df [ col ] = age_df [ col ] . str . replace ( '-' , '0' ) age_df [ col ] = age_df [ col ] . astype ( int ) def _age_bin ( age_df , age_bins ): \"\"\"Function sums the counts for corresponding age-bins and assigns them a column in age_df.\"\"\" for bin in age_bins : age_df [ f \" { bin [ 0 ] } - { bin [ 1 ] } \" ] = ( age_df . loc [:, bin [ 0 ]: bin [ 1 ]] . sum ( axis = 1 )) return age_df # create 90+ column for when there are more columns than 90 if len ( age_df . columns ) > 91 : # create 90+ column summing all those from 90 and above. age_df [ '90+' ] = age_df . iloc [:, 90 :] . sum ( axis = 1 ) age_df = _age_bin ( age_df , age_bins ) # drop the original age columns age_df . drop ( col_nms , axis = 1 , inplace = True ) # drop the columns that we are replacing with 90+ age_df . drop ( age_df . iloc [:, 19 :], axis = 1 , inplace = True ) # moving first column to last so 90+ at the end. temp_cols = age_df . columns . tolist () new_cols = temp_cols [ 1 :] + temp_cols [ 0 : 1 ] age_df = age_df [ new_cols ] else : age_df = _age_bin ( age_df , age_bins ) # drop the original age columns age_df . drop ( col_nms , axis = 1 , inplace = True ) # rename the 90+ column age_df . rename ( columns = { '90+-90+' : '90+' }, inplace = True ) # age df has now been binned and cleaned return round ( age_df )","title":"bin_pop_ages()"},{"location":"data_transform/#src.data_transform.convert_east_north","text":"Converts latitude and longitude coordinates to British National Grid Parameters: df ( pd . DataFrame ) \u2013 df including the longitude and latitude coordinates long(str) \u2013 The name of the longitude column in df lat ( str ) \u2013 The name of the latitude column in df Returns: \u2013 pd.DataFrame: dataframe including easting and northing coordinates. src/data_transform.py 438 439 440 441 442 443 444 445 446 447 448 449 def convert_east_north ( df , long , lat ): \"\"\" Converts latitude and longitude coordinates to British National Grid Args: df (pd.DataFrame): df including the longitude and latitude coordinates long(str): The name of the longitude column in df lat (str): The name of the latitude column in df Returns: pd.DataFrame: dataframe including easting and northing coordinates. \"\"\" df [ 'Easting' ], df [ 'Northing' ] = convert_bng ( df [ long ], df [ lat ]) return df","title":"convert_east_north()"},{"location":"data_transform/#src.data_transform.create_tiploc_col","text":"Creates a Tiploc column from the ATCOCode column, in the NaPTAN dataset. Parameters: naptan_df ( pd . Dataframe ) \u2013 Naptan dataset Returns: \u2013 pd.Dataframe (naptan_df): Naptan dataset with the new tiploc column \u2013 added for train stations src/data_transform.py 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 def create_tiploc_col ( naptan_df ): \"\"\"Creates a Tiploc column from the ATCOCode column, in the NaPTAN dataset. Args: naptan_df (pd.Dataframe): Naptan dataset Returns: pd.Dataframe (naptan_df): Naptan dataset with the new tiploc column added for train stations \"\"\" # Applying only to train stations, RLY is the stop type for train stations rail_filter = naptan_df . StopType == \"RLY\" # Create a new pd.Dataframe for Tiploc by extracting upto 7 alpha # characters tiploc_col = ( naptan_df . loc [ rail_filter ] . ATCOCode . str . extract ( r '([A-Za-z]{1,7})' ) ) tiploc_col . columns = [ \"tiploc_code\" ] # Merge the new Tiploc column with the naptan_df naptan_df = naptan_df . merge ( tiploc_col , how = 'left' , left_index = True , right_index = True ) return naptan_df","title":"create_tiploc_col()"},{"location":"data_transform/#src.data_transform.disab_dict","text":"Creates the dataframe including those who are and are not served by public transport and places it into a disability dictionary for each local authority of interest for the final csv output. Parameters: la_pop_df ( gpd . GeoDataFrame ) \u2013 GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df ( gpd . GeoDataFrame ) \u2013 A geodata frame with the points inside the polygon. disability_dict ( dict ) \u2013 Dictionary to store the disability dataframe. local_auth ( str ) \u2013 The local authority of interest. Returns: disability_dict ( dict ) \u2013 Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. src/data_transform.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def disab_dict ( la_pop_df , pop_in_poly_df , disability_dict , local_auth ): \"\"\"Creates the dataframe including those who are and are not served by public transport and places it into a disability dictionary for each local authority of interest for the final csv output. Args: la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df (gpd.GeoDataFrame): A geodata frame with the points inside the polygon. disability_dict (dict): Dictionary to store the disability dataframe. local_auth (str): The local authority of interest. Returns: disability_dict (dict): Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. \"\"\" # Calculating those served and not served by disability disab_cols = [ \"number_disabled\" ] disab_servd_df = served_proportions_disagg ( la_pop_df , pop_in_poly_df , disab_cols ) # Feeding the results to the reshaper disab_servd_df_out = do . reshape_for_output ( disab_servd_df , id_col = disab_cols [ 0 ], local_auth = local_auth , id_rename = \"Disability Status\" ) # The disability df is unusual. I think all rows correspond to people with # disabilities only. There is no \"not-disabled\" status here (I think) disab_servd_df_out . replace ( to_replace = \"number_disabled\" , value = \"Disabled\" , inplace = True ) # Calculating non-disabled people served and not served non_disab_cols = [ \"number_non-disabled\" ] non_disab_servd_df = served_proportions_disagg ( pop_df = la_pop_df , pop_in_poly_df = pop_in_poly_df , cols_lst = non_disab_cols ) # Feeding the results to the reshaper non_disab_servd_df_out = do . reshape_for_output ( non_disab_servd_df , id_col = disab_cols [ 0 ], local_auth = local_auth , id_rename = \"Disability Status\" ) # The disability df is unusual. I think all rows correspond to people with # disabilities only. There is no \"not-disabled\" status here (I think) non_disab_servd_df_out . replace ( to_replace = \"number_non-disabled\" , value = \"Non-disabled\" , inplace = True ) # Concatting non-disabled and disabled dataframes non_disab_disab_servd_df_out = pd . concat ( [ non_disab_servd_df_out , disab_servd_df_out ]) # Output this local auth's disab df to the dict disability_dict [ local_auth ] = non_disab_disab_servd_df_out return disability_dict","title":"disab_dict()"},{"location":"data_transform/#src.data_transform.disab_disagg","text":"Calculates number of people in the population that are classified as disabled or not disabled and this is merged onto the local authority population dataframe. Parameters: disability_df ( pd . DataFrame ) \u2013 Dataframe that includes disability estimates for each output area. la_pop_df ( gpd . GeoDataFrame ) \u2013 GeoPandas Dataframe that includes output area codes and population estimates. Returns: \u2013 gpd.GeoDataFrame: GeoPandas Dataframe that includes population estimates, geographical location, and proportion of disabled/non-disabled for each output area in the local authority chosen. src/data_transform.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def disab_disagg ( disability_df , la_pop_df ): \"\"\"Calculates number of people in the population that are classified as disabled or not disabled and this is merged onto the local authority population dataframe. Args: disability_df (pd.DataFrame): Dataframe that includes disability estimates for each output area. la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. Returns: gpd.GeoDataFrame: GeoPandas Dataframe that includes population estimates, geographical location, and proportion of disabled/non-disabled for each output area in the local authority chosen. \"\"\" # Getting the disab total disability_df [ \"disb_total\" ] = ( disability_df [ \"disab_ltd_lot\" ] + disability_df [ \"disab_ltd_little\" ]) # Calcualting the total \"non-disabled\" la_pop_only = la_pop_df [[ 'OA11CD' , 'pop_count' ]] disability_df = la_pop_only . merge ( disability_df , on = \"OA11CD\" ) # Putting the result back into the disability df disability_df [ \"non-disabled\" ] = disability_df [ \"pop_count\" ] - \\ disability_df [ 'disb_total' ] # Calculating the proportion of disabled people in each OA disability_df [ \"proportion_disabled\" ] = ( disability_df [ 'disb_total' ] / disability_df [ 'pop_count' ] ) # Calcualting the proportion of non-disabled people in each OA disability_df [ \"proportion_non-disabled\" ] = ( disability_df [ 'non-disabled' ] / disability_df [ 'pop_count' ] ) # Slice disability df that only has the proportion disabled column and the # OA11CD col disab_prop_df = disability_df [[ 'OA11CD' , 'proportion_disabled' , 'proportion_non-disabled' ]] # Merge the proportion disability df into main the pop df with a left join la_pop_df = la_pop_df . merge ( disab_prop_df , on = 'OA11CD' , how = \"left\" ) # Make the calculation of the number of people with disabilities in the # year of the population estimates la_pop_df [ \"number_disabled\" ] = ( round ( la_pop_df [ \"pop_count\" ] * la_pop_df [ \"proportion_disabled\" ]) ) # la_pop_df[\"number_disabled\"] = la_pop_df[\"number_disabled\"].astype(int) # Make the calculation of the number of non-disabled people in the year # of the population estimates la_pop_df [ \"number_non-disabled\" ] = ( round ( la_pop_df [ \"pop_count\" ] * la_pop_df [ \"proportion_non-disabled\" ]) ) la_pop_df [ \"number_non-disabled\" ] = la_pop_df [ \"number_non-disabled\" ] . astype ( int ) return la_pop_df","title":"disab_disagg()"},{"location":"data_transform/#src.data_transform.get_col_bins","text":"Function to group/bin the ages together in 5 year steps. Starts the sequence at age 0. Will return the ages in a list of tuples. The 0th position of the tuple being the lower limit of the age bin, the 1st position of the tuple being the upper limit. Parameters: col_nms ( list of str ) \u2013 a list of the age columns as strings. Returns: \u2013 list of tuples: a list of the ages with 5 year gaps. src/data_transform.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_col_bins ( col_nms : List [ str ]): \"\"\"Function to group/bin the ages together in 5 year steps. Starts the sequence at age 0. Will return the ages in a list of tuples. The 0th position of the tuple being the lower limit of the age bin, the 1st position of the tuple being the upper limit. Args: col_nms (list of str): a list of the age columns as strings. Returns: list of tuples: a list of the ages with 5 year gaps. \"\"\" # Make a lists of starting and finishing indexes cols_start = col_nms [ 0 :: 5 ] cols_fin = col_nms [ 4 :: 5 ] # Generating a list of tuples which will be the age groupings col_bins = [( s , f ) for s , f in zip ( cols_start , cols_fin )] # Again adding \"90+\", doubling it so it's doubled, like the other tuples col_bins . append (( cols_start [ - 1 :] * 2 )) # TODO: make this more intelligent. Only if there is one col name left # over it should be doubled. return col_bins","title":"get_col_bins()"},{"location":"data_transform/#src.data_transform.mid_year_age_estimates","text":"Takes mid-year population estimates for each small area in NI and uses proportions to calculate mid-year estimates for each age. Parameters: age_df ( pd . DataFrame ) \u2013 Census 2011 age estimates dataframe pop_estimates_df ( pd . DataFrame ) \u2013 population estimates for each small area dataframe pop_year ( str ) \u2013 population year src/data_transform.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def mid_year_age_estimates ( age_df , pop_estimates_df , pop_year ): \"\"\" Takes mid-year population estimates for each small area in NI and uses proportions to calculate mid-year estimates for each age. Args: age_df (pd.DataFrame): Census 2011 age estimates dataframe pop_estimates_df (pd.DataFrame): population estimates for each small area dataframe pop_year (str): population year \"\"\" # get all age columns in a list age_cols = [ str ( y ) for y in range ( 101 )] # iterate through each age for age in range ( len ( age_cols )): # calculates the proportions for each age and each SA code age_df [ age_cols [ age ]] = age_df [ age_cols [ age ]] / age_df [ 'All usual residents' ] age_df . drop ([ 'SA' , 'All usual residents' ], axis = 1 , inplace = True ) # merges pop df and proportions together pop_estimates_df = pop_estimates_df . merge ( age_df . reset_index (), left_on = 'Area_Code' , right_on = 'SA Code' , how = 'left' ) # calculates pop estimates for each age in each small area using proportions for age in range ( len ( age_cols )): pop_estimates_df [ age_cols [ age ]] = pop_estimates_df [ age_cols [ age ]] * pop_estimates_df [ pop_year ] pop_estimates_df . set_index ( 'Area_Code' , inplace = True ) return pop_estimates_df","title":"mid_year_age_estimates()"},{"location":"data_transform/#src.data_transform.served_proportions_disagg","text":"Calculates the number of people in each category, as specified by the column (e.g age range, or disability status) who are served and not served by public transport, and gives those as a proportion of the total. Note: the numeric values in the dataframe are return as strings for formatting reasons Parameters: pop_df ( pd . DataFrame ) \u2013 population dataframe. pop_in_poly_df ( pd . DataFrame ) \u2013 dataframe resulting in the points in polygons enquiry to count (sum) the population within the service area polygon. cols_lst ( List [ str ] ) \u2013 a list of the column names in the population dataframe supplied which contain population figures, and are to be summed and assessed as served/unserved by public transport. Returns: \u2013 pd.DataFrame: a dataframe summarising \u2013 i) the total number of people that column (e.g. age range, sex) \u2013 ii) the number served by public transport \u2013 iii) the proportion who are served by public transport \u2013 iv) the proportion who are not served by public transport src/data_transform.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def served_proportions_disagg ( pop_df : pd . DataFrame , pop_in_poly_df : pd . DataFrame , cols_lst : List [ str ]): \"\"\"Calculates the number of people in each category, as specified by the column (e.g age range, or disability status) who are served and not served by public transport, and gives those as a proportion of the total. Note: the numeric values in the dataframe are return as strings for formatting reasons Args: pop_df (pd.DataFrame): population dataframe. pop_in_poly_df (pd.DataFrame): dataframe resulting in the points in polygons enquiry to count (sum) the population within the service area polygon. cols_lst (List[str]): a list of the column names in the population dataframe supplied which contain population figures, and are to be summed and assessed as served/unserved by public transport. Returns: pd.DataFrame: a dataframe summarising i) the total number of people that column (e.g. age range, sex) ii) the number served by public transport iii) the proportion who are served by public transport iv) the proportion who are not served by public transport \"\"\" # First list the age bin columns pop_sums = {} for col in cols_lst : # Total pop total_pop = int ( pop_df [ col ] . sum ()) # Served pop servd_pop = int ( pop_in_poly_df [ col ] . sum ()) # Unserved pop unsrvd_pop = int ( total_pop - servd_pop ) if total_pop == 0 : # If the total population for that column is 0 # this standard of zeros and Nones is returned pop_sums [ col ] = { \"Total\" : str ( total_pop ), \"Served\" : str ( servd_pop ), \"Unserved\" : str ( unsrvd_pop ), \"Percentage served\" : \"None\" , \"Percentage unserved\" : \"None\" } elif total_pop > 0 : pop_sums [ col ] = _calc_proprtn_srvd_unsrvd ( total_pop , servd_pop , unsrvd_pop ) # Make a df from the total and served pop tot_servd_df = pd . DataFrame ( pop_sums ) return tot_servd_df","title":"served_proportions_disagg()"},{"location":"data_transform/#src.data_transform.slice_age_df","text":"Slices a dataframe according to the list of column names provided. Parameters: df ( pd . DataFrame ) \u2013 DataFrame to be sliced. col_nms ( List [ str ] ) \u2013 column names as string in a list. Returns: \u2013 pd.DataFrame: A dataframe sliced down to only the columns required. src/data_transform.py 26 27 28 29 30 31 32 33 34 35 36 37 def slice_age_df ( df : pd . DataFrame , col_nms : List [ str ]): \"\"\"Slices a dataframe according to the list of column names provided. Args: df (pd.DataFrame): DataFrame to be sliced. col_nms (List[str]): column names as string in a list. Returns: pd.DataFrame: A dataframe sliced down to only the columns required. \"\"\" age_df = df . loc [:, col_nms ] return age_df","title":"slice_age_df()"},{"location":"data_transform/#src.data_transform.urban_rural_results","text":"Creates two dataframes, urban and rural and classifies proportion of people served and not served by public transport. This is placed into a urban rural dictionary for each local authority of interest for the final csv output. Parameters: la_pop_df ( gpd . GeoDataFrame ) \u2013 GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df ( gpd . GeoDataFrame ) \u2013 A geodata frame with the points inside the polygon. urb_rur_dict ( dict ) \u2013 Dictionary to store the urban rural dataframe. local_auth ( str ) \u2013 The local authority of interest. Returns: urb_rur_dict ( dict ) \u2013 Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. src/data_transform.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def urban_rural_results ( la_pop_df , pop_in_poly_df , urb_rur_dict , local_auth ): \"\"\" Creates two dataframes, urban and rural and classifies proportion of people served and not served by public transport. This is placed into a urban rural dictionary for each local authority of interest for the final csv output. Args: la_pop_df (gpd.GeoDataFrame): GeoPandas Dataframe that includes output area codes and population estimates. pop_in_poly_df (gpd.GeoDataFrame): A geodata frame with the points inside the polygon. urb_rur_dict (dict): Dictionary to store the urban rural dataframe. local_auth (str): The local authority of interest. Returns: urb_rur_dict (dict): Dictionary with a disability total dataframe for unserved and served populations for all given local authorities. \"\"\" # Urban/Rural disaggregation # split into two different dataframes urb_df = la_pop_df [ la_pop_df . urb_rur_class == \"urban\" ] rur_df = la_pop_df [ la_pop_df . urb_rur_class == \"rural\" ] urb_df_poly = pop_in_poly_df [ pop_in_poly_df . urb_rur_class == \"urban\" ] rur_df_poly = pop_in_poly_df [ pop_in_poly_df . urb_rur_class == \"rural\" ] urb_servd_df = served_proportions_disagg ( pop_df = urb_df , pop_in_poly_df = urb_df_poly , cols_lst = [ 'pop_count' ]) rur_servd_df = served_proportions_disagg ( pop_df = rur_df , pop_in_poly_df = rur_df_poly , cols_lst = [ 'pop_count' ]) # Renaming pop_count to either urban or rural urb_servd_df . rename ( columns = { \"pop_count\" : \"Urban\" }, inplace = True ) rur_servd_df . rename ( columns = { \"pop_count\" : \"Rural\" }, inplace = True ) # Sending each to reshaper urb_servd_df_out = do . reshape_for_output ( urb_servd_df , id_col = \"Urban\" , local_auth = local_auth ) rur_servd_df_out = do . reshape_for_output ( rur_servd_df , id_col = \"Rural\" , local_auth = local_auth ) # Renaming their columns to Urban/Rural urb_servd_df_out . rename ( columns = { \"Urban\" : \"Urban/Rural\" }, inplace = True ) rur_servd_df_out . rename ( columns = { \"Rural\" : \"Urban/Rural\" }, inplace = True ) # Combining urban and rural dfs urb_rur_servd_df_out = pd . concat ([ urb_servd_df_out , rur_servd_df_out ]) # Output this iteration's urb and rur df to the dict urb_rur_dict [ local_auth ] = urb_rur_servd_df_out return urb_rur_dict","title":"urban_rural_results()"},{"location":"developers_guide/","text":"For Developers Requirements A number of problems with dependencies have been experienced while developing this, so it is strongly recommended that you use a virtual environment (either conda or venv) and use the provided requirements.txt to install the needed versions of packages. Note for ONS staff: It is unlikely that you will be able to install all the needed dependencies to run this script, therefore it is recommended that your devlopment work is carried out on an off-network computer. Before starting this process, please ensure that Anaconda3 and Git are installed. We recommend running the script using VSCode, as this is what we use in these instructions, and is downloadable here . If you are using Windows, you will have to add the conda and python path into your Windows Path environment. For guidance, follow the tutorial here . Cloning the repository The first step is setting up your SSH key for GitHub. The process will slightly vary depending on what OS you are running from. Here are useful tutorials for Windows 10 or Mac and Linux . You should now have your SSH key set up. To clone the repository, we need to first go to the project directory (where you would like it saved on your local drive). $ cd project-directory Then activate use the SSH address to clone the repository $ git clone SSH_address You can then open the folder SDG_11.2.1 within VSCode using \"Open Folder\" in Source Control. Create an environment Virtual environments are extremely useful when working on different projects as they can be set up in a way to only have packages installed in that environment and not globally - it does not affect base Python installation in any way. Create an environment called \"SDG_11.2.1\" with the version of Python that this was developed in conda create --name SDG_11.2.1 python=3.8 Troubleshooting: When creating the environment on Windows, it might fail with a HTTP error when trying to fetch package metadata. Following these steps should resolve the issue. Activate the environment First go to the project directory/wherever you have saved to on your local drive. On Windows this may look like: $ cd C:\\Users\\name\\project-directory Then activate the environment $ conda activate SDG_11.2.1 The above command will not work for Windows unless you have the Conda path linked to your Windows path environment (see this tutorial ). If you do not have the Conda path linked, you can use the following for Windows: $ activate SDG_11.2.1 Then you should see the environment name in brackets before the prompt, similar to: (SDG_11.2.1) $ Troubleshooting: Activating the environment in the Powershell terminal does not work for Windows. Set the default terminal to Command Prompt and activate from here. Press CTRL+Shift+P and search for 'Terminal: Configure Terminal Settings'. Under the 'Terminal > External: Windows Exec' section enter the path to your cmd. E.g. C:\\WINDOWS\\System32\\cmd.exe Make sure you are using the correct Python interpreter by checking your Python path: In Linux: $ which python And in Windows use the following command in cmd or Anaconda prompt: $ where python Which should return something like: C:\\Python36\\envs\\SDG_11.2.1\\python.exe Showing your are using the Python from the virtual environment, not the base installation of Python. Installing dependencies First, ensure you are in the project directory $ cd C:\\Users\\name\\project-directory To install the requirements write conda install --file requirements.txt This may throw an error if you do not have all the packages required PackageNotFoundError: The following packages are not available from current channels If this does, write the following with the package names that the error has shown you are missing: pip install packagename The script should now be set up to use. Setting Git configuration To be able to contribute to the project via Git, you will need to add the email and user name associated to your account to the config file git config --global user.email \"email\" git config --global user.name \"username\"","title":"For Developers"},{"location":"developers_guide/#for-developers","text":"","title":"For Developers"},{"location":"developers_guide/#requirements","text":"A number of problems with dependencies have been experienced while developing this, so it is strongly recommended that you use a virtual environment (either conda or venv) and use the provided requirements.txt to install the needed versions of packages. Note for ONS staff: It is unlikely that you will be able to install all the needed dependencies to run this script, therefore it is recommended that your devlopment work is carried out on an off-network computer. Before starting this process, please ensure that Anaconda3 and Git are installed. We recommend running the script using VSCode, as this is what we use in these instructions, and is downloadable here . If you are using Windows, you will have to add the conda and python path into your Windows Path environment. For guidance, follow the tutorial here .","title":"Requirements"},{"location":"developers_guide/#cloning-the-repository","text":"The first step is setting up your SSH key for GitHub. The process will slightly vary depending on what OS you are running from. Here are useful tutorials for Windows 10 or Mac and Linux . You should now have your SSH key set up. To clone the repository, we need to first go to the project directory (where you would like it saved on your local drive). $ cd project-directory Then activate use the SSH address to clone the repository $ git clone SSH_address You can then open the folder SDG_11.2.1 within VSCode using \"Open Folder\" in Source Control.","title":"Cloning the repository"},{"location":"developers_guide/#create-an-environment","text":"Virtual environments are extremely useful when working on different projects as they can be set up in a way to only have packages installed in that environment and not globally - it does not affect base Python installation in any way. Create an environment called \"SDG_11.2.1\" with the version of Python that this was developed in conda create --name SDG_11.2.1 python=3.8 Troubleshooting: When creating the environment on Windows, it might fail with a HTTP error when trying to fetch package metadata. Following these steps should resolve the issue.","title":"Create an environment"},{"location":"developers_guide/#activate-the-environment","text":"First go to the project directory/wherever you have saved to on your local drive. On Windows this may look like: $ cd C:\\Users\\name\\project-directory Then activate the environment $ conda activate SDG_11.2.1 The above command will not work for Windows unless you have the Conda path linked to your Windows path environment (see this tutorial ). If you do not have the Conda path linked, you can use the following for Windows: $ activate SDG_11.2.1 Then you should see the environment name in brackets before the prompt, similar to: (SDG_11.2.1) $ Troubleshooting: Activating the environment in the Powershell terminal does not work for Windows. Set the default terminal to Command Prompt and activate from here. Press CTRL+Shift+P and search for 'Terminal: Configure Terminal Settings'. Under the 'Terminal > External: Windows Exec' section enter the path to your cmd. E.g. C:\\WINDOWS\\System32\\cmd.exe Make sure you are using the correct Python interpreter by checking your Python path: In Linux: $ which python And in Windows use the following command in cmd or Anaconda prompt: $ where python Which should return something like: C:\\Python36\\envs\\SDG_11.2.1\\python.exe Showing your are using the Python from the virtual environment, not the base installation of Python.","title":"Activate the environment"},{"location":"developers_guide/#installing-dependencies","text":"First, ensure you are in the project directory $ cd C:\\Users\\name\\project-directory To install the requirements write conda install --file requirements.txt This may throw an error if you do not have all the packages required PackageNotFoundError: The following packages are not available from current channels If this does, write the following with the package names that the error has shown you are missing: pip install packagename The script should now be set up to use.","title":"Installing dependencies"},{"location":"developers_guide/#setting-git-configuration","text":"To be able to contribute to the project via Git, you will need to add the email and user name associated to your account to the config file git config --global user.email \"email\" git config --global user.name \"username\"","title":"Setting Git configuration"},{"location":"for_developers/","text":"For Developers Requirements A number of problems with dependencies have been experienced while developing this, so it is strongly recommended that you use a virtual environment (either conda or venv) and use the provided requirements.txt to install the needed versions of packages. Note for ONS staff: It is unlikely that you will be able to install all the needed dependencies to run this script, therefore it is recommended that your devlopment work is carried out on an off-network computer. Before starting this process, please ensure that Anaconda3 and Git are installed. We recommend running the script using VSCode, as this is what we use in these instructions, and is downloadable here . If you are using Windows, you will have to add the conda and python path into your Windows Path environment. For guidance, follow the tutorial here . Cloning the repository The first step is setting up your SSH key for GitHub. The process will slightly vary depending on what OS you are running from. Here are useful tutorials for Windows 10 or Mac and Linux . You should now have your SSH key set up. To clone the repository, we need to first go to the project directory (where you would like it saved on your local drive). $ cd project-directory Then activate use the SSH address to clone the repository $ git clone SSH_address You can then open the folder SDG_11.2.1 within VSCode using \"Open Folder\" in Source Control. Create an environment Virtual environments are extremely useful when working on different projects as they can be set up in a way to only have packages installed in that environment and not globally - it does not affect base Python installation in any way. Create an environment called \"SDG_11.2.1\" with the version of Python that this was developed in conda create --name SDG_11.2.1 python=3.8 Troubleshooting: When creating the environment on Windows, it might fail with a HTTP error when trying to fetch package metadata. Following these steps should resolve the issue. Activate the environment First go to the project directory/wherever you have saved to on your local drive. On Windows this may look like: $ cd C:\\Users\\name\\project-directory Then activate the environment $ conda activate SDG_11.2.1 The above command will not work for Windows unless you have the Conda path linked to your Windows path environment (see this tutorial ). If you do not have the Conda path linked, you can use the following for Windows: $ activate SDG_11.2.1 Then you should see the environment name in brackets before the prompt, similar to: (SDG_11.2.1) $ Troubleshooting: Activating the environment in the Powershell terminal does not work for Windows. Set the default terminal to Command Prompt and activate from here. Press CTRL+Shift+P and search for 'Terminal: Configure Terminal Settings'. Under the 'Terminal > External: Windows Exec' section enter the path to your cmd. E.g. C:\\WINDOWS\\System32\\cmd.exe Make sure you are using the correct Python interpreter by checking your Python path: In Linux: $ which python And in Windows use the following command in cmd or Anaconda prompt: $ where python Which should return something like: C:\\Python36\\envs\\SDG_11.2.1\\python.exe Showing your are using the Python from the virtual environment, not the base installation of Python. Installing dependencies First, ensure you are in the project directory $ cd C:\\Users\\name\\project-directory To install the requirements write conda install --file requirements.txt This may throw an error if you do not have all the packages required PackageNotFoundError: The following packages are not available from current channels If this does, write the following with the package names that the error has shown you are missing: pip install packagename The script should now be set up to use. Setting Git configuration To be able to contribute to the project via Git, you will need to add the email and user name associated to your account to the config file git config --global user.email \"email\" git config --global user.name \"username\"","title":"For Developers"},{"location":"for_developers/#for-developers","text":"","title":"For Developers"},{"location":"for_developers/#requirements","text":"A number of problems with dependencies have been experienced while developing this, so it is strongly recommended that you use a virtual environment (either conda or venv) and use the provided requirements.txt to install the needed versions of packages. Note for ONS staff: It is unlikely that you will be able to install all the needed dependencies to run this script, therefore it is recommended that your devlopment work is carried out on an off-network computer. Before starting this process, please ensure that Anaconda3 and Git are installed. We recommend running the script using VSCode, as this is what we use in these instructions, and is downloadable here . If you are using Windows, you will have to add the conda and python path into your Windows Path environment. For guidance, follow the tutorial here .","title":"Requirements"},{"location":"for_developers/#cloning-the-repository","text":"The first step is setting up your SSH key for GitHub. The process will slightly vary depending on what OS you are running from. Here are useful tutorials for Windows 10 or Mac and Linux . You should now have your SSH key set up. To clone the repository, we need to first go to the project directory (where you would like it saved on your local drive). $ cd project-directory Then activate use the SSH address to clone the repository $ git clone SSH_address You can then open the folder SDG_11.2.1 within VSCode using \"Open Folder\" in Source Control.","title":"Cloning the repository"},{"location":"for_developers/#create-an-environment","text":"Virtual environments are extremely useful when working on different projects as they can be set up in a way to only have packages installed in that environment and not globally - it does not affect base Python installation in any way. Create an environment called \"SDG_11.2.1\" with the version of Python that this was developed in conda create --name SDG_11.2.1 python=3.8 Troubleshooting: When creating the environment on Windows, it might fail with a HTTP error when trying to fetch package metadata. Following these steps should resolve the issue.","title":"Create an environment"},{"location":"for_developers/#activate-the-environment","text":"First go to the project directory/wherever you have saved to on your local drive. On Windows this may look like: $ cd C:\\Users\\name\\project-directory Then activate the environment $ conda activate SDG_11.2.1 The above command will not work for Windows unless you have the Conda path linked to your Windows path environment (see this tutorial ). If you do not have the Conda path linked, you can use the following for Windows: $ activate SDG_11.2.1 Then you should see the environment name in brackets before the prompt, similar to: (SDG_11.2.1) $ Troubleshooting: Activating the environment in the Powershell terminal does not work for Windows. Set the default terminal to Command Prompt and activate from here. Press CTRL+Shift+P and search for 'Terminal: Configure Terminal Settings'. Under the 'Terminal > External: Windows Exec' section enter the path to your cmd. E.g. C:\\WINDOWS\\System32\\cmd.exe Make sure you are using the correct Python interpreter by checking your Python path: In Linux: $ which python And in Windows use the following command in cmd or Anaconda prompt: $ where python Which should return something like: C:\\Python36\\envs\\SDG_11.2.1\\python.exe Showing your are using the Python from the virtual environment, not the base installation of Python.","title":"Activate the environment"},{"location":"for_developers/#installing-dependencies","text":"First, ensure you are in the project directory $ cd C:\\Users\\name\\project-directory To install the requirements write conda install --file requirements.txt This may throw an error if you do not have all the packages required PackageNotFoundError: The following packages are not available from current channels If this does, write the following with the package names that the error has shown you are missing: pip install packagename The script should now be set up to use.","title":"Installing dependencies"},{"location":"for_developers/#setting-git-configuration","text":"To be able to contribute to the project via Git, you will need to add the email and user name associated to your account to the config file git config --global user.email \"email\" git config --global user.name \"username\"","title":"Setting Git configuration"},{"location":"geospatial_mods/","text":"buffer_points ( geo_df ) Creates a 500m or 1000m buffer around points. Draws 500m if the capacity_type is low Draws 1000m if the capacity_type is high Puts the results into a new column called \"geometry\" As 'epsg:27700' projections units of km, 500m is 0.5km. Parameters: geo_df ( gpd . DataFrame ) \u2013 Data frame of points to be buffered including a column with the capacity_type for each point. Returns: gpd . GeoDataFrame \u2013 gpd.DataFrame: A dataframe of polygons create from the buffer. src/geospatial_mods.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def buffer_points ( geo_df : gpd . GeoDataFrame ) -> gpd . GeoDataFrame : \"\"\"Creates a 500m or 1000m buffer around points. Draws 500m if the capacity_type is low Draws 1000m if the capacity_type is high Puts the results into a new column called \"geometry\" As 'epsg:27700' projections units of km, 500m is 0.5km. Args: geo_df (gpd.DataFrame): Data frame of points to be buffered including a column with the capacity_type for each point. Returns: gpd.DataFrame: A dataframe of polygons create from the buffer. \"\"\" # raise an error if high or low not correct capacity type for value in geo_df [ \"capacity_type\" ]: if value not in [ \"high\" , \"low\" ]: raise ValueError ( f \"\"\" { value } is not a valid capacity type, should be either high or low\"\"\" ) # conditions conditions = [ geo_df [ 'capacity_type' ] == \"low\" , geo_df [ 'capacity_type' ] == \"high\" ] # values values = [ geo_df . geometry . buffer ( LOWERBUFFER ), geo_df . geometry . buffer ( UPPERBUFFER )] # apply conditions geo_df [ 'geometry' ] = np . select ( condlist = conditions , choicelist = values ) return geo_df find_points_in_poly ( geo_df , polygon_obj ) Find points in polygon using geopandas' spatial join which joins the supplied geo_df (as left_df) and the polygon (as right_df). Then drops all rows where the point is not in the polygon (based on column index_right not being NaN). Finally it drop all column names from that were created in the join, leaving only the columns of the original geo_df. Parameters: geo_df ( gpg . DatFrame ) \u2013 a geo pandas dataframe. polygon_obj ( str ) \u2013 a geopandas dataframe with a polygon column. Returns: \u2013 gpd.GeoDataFrame: A geodata frame with the points inside the supplied \u2013 polygon. src/geospatial_mods.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def find_points_in_poly ( geo_df : gpd . GeoDataFrame , polygon_obj ): \"\"\"Find points in polygon using geopandas' spatial join which joins the supplied geo_df (as left_df) and the polygon (as right_df). Then drops all rows where the point is not in the polygon (based on column index_right not being NaN). Finally it drop all column names from that were created in the join, leaving only the columns of the original geo_df. Args: geo_df (gpg.DatFrame): a geo pandas dataframe. polygon_obj (str): a geopandas dataframe with a polygon column. Returns: gpd.GeoDataFrame: A geodata frame with the points inside the supplied polygon. \"\"\" wanted_cols = geo_df . columns . to_list () joined_df = ( gpd . sjoin ( geo_df , polygon_obj , how = 'left' , predicate = 'intersects' )) # op = 'within' filtered_df = ( joined_df [ joined_df [ 'index_right' ] . notna ()]) filtered_df = filtered_df [ wanted_cols ] return filtered_df get_polygons_of_loccode ( geo_df , dissolveby = 'OA11CD' , search = None ) Gets the polygon for a place based on it name, LSOA code or OA code. Parameters: geo_df ( gpd . GeoDataFrame ) \u2013 Lookup geospatial data frame. loc_code ( str ) \u2013 Can be one of LSOA11CD, OA11CD or LSOA11NM. OA11CD by default. search ( str ) \u2013 Search terms to find in the LSOA11NM column. Only needed if intending to dissolve on a name in the LSOA11NM column. Defualt is None. Returns: gpd . GeoDataFrame \u2013 gpd.DataFrame: GeoDataFrame with multipolygons agregated on LSOA, OA code, or a search in the LSOA11NM column. src/geospatial_mods.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def get_polygons_of_loccode ( geo_df : gpd . GeoDataFrame , dissolveby = 'OA11CD' , search = None ) -> gpd . GeoDataFrame : \"\"\"Gets the polygon for a place based on it name, LSOA code or OA code. Args: geo_df (gpd.GeoDataFrame): Lookup geospatial data frame. loc_code (str): Can be one of LSOA11CD, OA11CD or LSOA11NM. OA11CD by default. search (str): Search terms to find in the LSOA11NM column. Only needed if intending to dissolve on a name in the LSOA11NM column. Defualt is None. Returns: gpd.DataFrame: GeoDataFrame with multipolygons agregated on LSOA, OA code, or a search in the LSOA11NM column. \"\"\" if dissolveby in [ 'LSOA11CD' , 'OA11CD' ]: polygon_df = geo_df . dissolve ( by = dissolveby ) else : filtered_df = geo_df [ geo_df [ f ' { dissolveby } ' ] . str . contains ( search )] filtered_df . insert ( 0 , 'place_name' , search ) polygon_df = filtered_df . dissolve ( by = 'place_name' ) polygon_df = gpd . GeoDataFrame ( polygon_df . pop ( 'geometry' )) return polygon_df","title":"Geospatial mods"},{"location":"geospatial_mods/#src.geospatial_mods.buffer_points","text":"Creates a 500m or 1000m buffer around points. Draws 500m if the capacity_type is low Draws 1000m if the capacity_type is high Puts the results into a new column called \"geometry\" As 'epsg:27700' projections units of km, 500m is 0.5km. Parameters: geo_df ( gpd . DataFrame ) \u2013 Data frame of points to be buffered including a column with the capacity_type for each point. Returns: gpd . GeoDataFrame \u2013 gpd.DataFrame: A dataframe of polygons create from the buffer. src/geospatial_mods.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def buffer_points ( geo_df : gpd . GeoDataFrame ) -> gpd . GeoDataFrame : \"\"\"Creates a 500m or 1000m buffer around points. Draws 500m if the capacity_type is low Draws 1000m if the capacity_type is high Puts the results into a new column called \"geometry\" As 'epsg:27700' projections units of km, 500m is 0.5km. Args: geo_df (gpd.DataFrame): Data frame of points to be buffered including a column with the capacity_type for each point. Returns: gpd.DataFrame: A dataframe of polygons create from the buffer. \"\"\" # raise an error if high or low not correct capacity type for value in geo_df [ \"capacity_type\" ]: if value not in [ \"high\" , \"low\" ]: raise ValueError ( f \"\"\" { value } is not a valid capacity type, should be either high or low\"\"\" ) # conditions conditions = [ geo_df [ 'capacity_type' ] == \"low\" , geo_df [ 'capacity_type' ] == \"high\" ] # values values = [ geo_df . geometry . buffer ( LOWERBUFFER ), geo_df . geometry . buffer ( UPPERBUFFER )] # apply conditions geo_df [ 'geometry' ] = np . select ( condlist = conditions , choicelist = values ) return geo_df","title":"buffer_points()"},{"location":"geospatial_mods/#src.geospatial_mods.find_points_in_poly","text":"Find points in polygon using geopandas' spatial join which joins the supplied geo_df (as left_df) and the polygon (as right_df). Then drops all rows where the point is not in the polygon (based on column index_right not being NaN). Finally it drop all column names from that were created in the join, leaving only the columns of the original geo_df. Parameters: geo_df ( gpg . DatFrame ) \u2013 a geo pandas dataframe. polygon_obj ( str ) \u2013 a geopandas dataframe with a polygon column. Returns: \u2013 gpd.GeoDataFrame: A geodata frame with the points inside the supplied \u2013 polygon. src/geospatial_mods.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def find_points_in_poly ( geo_df : gpd . GeoDataFrame , polygon_obj ): \"\"\"Find points in polygon using geopandas' spatial join which joins the supplied geo_df (as left_df) and the polygon (as right_df). Then drops all rows where the point is not in the polygon (based on column index_right not being NaN). Finally it drop all column names from that were created in the join, leaving only the columns of the original geo_df. Args: geo_df (gpg.DatFrame): a geo pandas dataframe. polygon_obj (str): a geopandas dataframe with a polygon column. Returns: gpd.GeoDataFrame: A geodata frame with the points inside the supplied polygon. \"\"\" wanted_cols = geo_df . columns . to_list () joined_df = ( gpd . sjoin ( geo_df , polygon_obj , how = 'left' , predicate = 'intersects' )) # op = 'within' filtered_df = ( joined_df [ joined_df [ 'index_right' ] . notna ()]) filtered_df = filtered_df [ wanted_cols ] return filtered_df","title":"find_points_in_poly()"},{"location":"geospatial_mods/#src.geospatial_mods.get_polygons_of_loccode","text":"Gets the polygon for a place based on it name, LSOA code or OA code. Parameters: geo_df ( gpd . GeoDataFrame ) \u2013 Lookup geospatial data frame. loc_code ( str ) \u2013 Can be one of LSOA11CD, OA11CD or LSOA11NM. OA11CD by default. search ( str ) \u2013 Search terms to find in the LSOA11NM column. Only needed if intending to dissolve on a name in the LSOA11NM column. Defualt is None. Returns: gpd . GeoDataFrame \u2013 gpd.DataFrame: GeoDataFrame with multipolygons agregated on LSOA, OA code, or a search in the LSOA11NM column. src/geospatial_mods.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def get_polygons_of_loccode ( geo_df : gpd . GeoDataFrame , dissolveby = 'OA11CD' , search = None ) -> gpd . GeoDataFrame : \"\"\"Gets the polygon for a place based on it name, LSOA code or OA code. Args: geo_df (gpd.GeoDataFrame): Lookup geospatial data frame. loc_code (str): Can be one of LSOA11CD, OA11CD or LSOA11NM. OA11CD by default. search (str): Search terms to find in the LSOA11NM column. Only needed if intending to dissolve on a name in the LSOA11NM column. Defualt is None. Returns: gpd.DataFrame: GeoDataFrame with multipolygons agregated on LSOA, OA code, or a search in the LSOA11NM column. \"\"\" if dissolveby in [ 'LSOA11CD' , 'OA11CD' ]: polygon_df = geo_df . dissolve ( by = dissolveby ) else : filtered_df = geo_df [ geo_df [ f ' { dissolveby } ' ] . str . contains ( search )] filtered_df . insert ( 0 , 'place_name' , search ) polygon_df = filtered_df . dissolve ( by = 'place_name' ) polygon_df = gpd . GeoDataFrame ( polygon_df . pop ( 'geometry' )) return polygon_df","title":"get_polygons_of_loccode()"},{"location":"main/","text":"The main pipeline sits here.","title":"Main"},{"location":"methodology/","text":"Calculation Methodology of Indicator 11.2.1 Version 1.0 James Westwood, Antonio Felton, Paige Hunter and Nathan Shaw. Introduction Our team has calculated data for transport accessibility across the UK following, as closely as we could, the method in the UN Metadata for the SDG indicator 11.2.1 . The following is a write up of the methodology we employed to make these calculations. In this methodology writeup we aim to accurately reflect the method used in version 1.0 of our project; read more about versions below in the section \u201cProject Versions\u201d. Acknowledgement The main support for this project on both methodology and finding appropriate data sources has come from the geospatial department, in particular Musa Chirikeni, to whom we are very grateful. Other help has come from Michael Hodge who laid out the initial method we might approach in Python, and from the SDG data team who have been supportive data end users. We also thank Mark Simons who contributed to data management code in early 2022. Background The statistics calculated for this project are to be used as an indicator of the UK\u2019s progress on target 11.2 of the Sustainable Development Goals (SDGs). The SDGs are a set of 17 Goals which 193 Member states signed up to in order to coordinate efforts on more sustainable and inclusive national development. The goals cover areas such as poverty, hunger, equality and the environment. Goal 11, which the indicator we are calculating for falls under, is concerned with Sustainable Cities. The data our team produces will be available on the UK data platform for the SDG indicators , specifically on this page . Project history The project was initiated in late 2019 by James Westwood with some initial help from the UK SDG data team and Michael Hodge from the Data Science Campus, who provided guidance on the tech that would need to be used and a rough step-by-step calculation process. Project Versions The project is managed using version control on the Github platform. One feature of the version controlling using Github is that we can make releases. At the time of writing we have not made any releases of the code, but when we feel that all features to calculate the statistics required by the end user have been written into our code, we will release version 1.0. Each version has a project board, wherein we group the issues describing the work that needs to be done to develop each feature required. Project boards are hosted on Github. Version Focus Example features 1.0 Fully working reproducible calculation of transport availability across the UK Calculate the proportion of the population within 500m of a public transport access point for the whole of the UK. Disaggregate the above number by sex, age, disability status Be auditable and reproducable 1.1 Quality Assurance Phase Updates to code from feedback (e.g. from data team and topic expert) Data cleaning on import Validation on import Unit tests for functions 1.2 Enhanced functions and calculation Focus on enhancements to the calculation - Improve existing functions, making them more generic and robust - Look into improving geographical accuracy with more granular calculations - Improve the analysis with more disaggregations. 1.3 Optimise Computation Focus on enhancements to the functioning of the code: Better data management (e.g. SQL tables) Refactoring code Code optimization (speed, memory) vectorised calculation, Calculation process Process diagram Main method of calculation The key step of calculation is a spatial join between geolocated urban population data and public transport service areas. A discussion of how each component of that join was calculated from their component parts follows. Geocoding the population data The questions we are trying to answer in this analysis is \u201cWhat proportion of people in the UK have access to public transport access points from their home?\u201d. And then the same question for people in each age, sex and disability category. To make the calculation as accurate as possible, we sourced the most granularly geolocated population data possible. Ideally, this would have been right down to the individual place of living (house/flat etc.) but such granularity is not available publicly, as it would be disclosive of individuals\u2019 living situations. The most granular level published that we are currently aware of is the \u201coutput area\u201d (OA) data, which derives from the census. Output areas (more fully defined and discussed in the data section of this methodology write up) are a conglomerate of the postcodes. Those postcodes are joined together on the basis that they are: Adjacent Of the same urban/rural classification In order to calculate the distance from the place any person lives, to a public transport access node (station or stop) we need a point-geometry for their residence. As we cannot geolocate individual residencies, we used an approximation in the form of population weighted centroids (PWCs). PWCs are a population-weighted location (x,y point) at which is a geospatial mean of the locations of all residences in that output area. The PWC, while extremely helpful in giving us a geospatial point to define where the population of any output area lives, is an approximation and is only used due to limitations in our data. Joining the data Population weighted centroid (PWC) data is geolocated with x,y coordinates and has a common data column with the output area data (output area code, \u201cOA11CD\u201d) so a table join can be performed on those datasets. Then the entire population of any output area is approximated to live at the centroid. Limitations and improvements Our team recognises that PWCs do not accurately represent the location of any individual\u2019s place of residence and we intend to research methods of making calculations at even more granular levels, such as postcodes or smaller. Currently however we agreed with the our data end user and the geospatial department that the method to be used for version 1.0 will use the described approximation to geolocate the population. Delimitation of urban areas As Target 11 is concerned with sustainable urban environments, we ultimately had to select only urban areas and exclude rural areas from our analysis. Urban areas are defined as Ordnance Survey mapping that have resident populations above 10,000 people (2011 Census) and the methodology is available here . The urban/rural lookup data was sourced from ONS\u2019s Geography Portal and it provides a classification of each output area as either urban or rural categories which are further subdivided as follows. Classification Description Code Urban Major Conurbation A1 Urban Minor Conurbation B1 Urban City and Town C1 Urban City and Town in a Sparse Setting C2 Rural Town and Fringe D1 Rural Town and Fringe in a Sparse Setting D2 Rural Village E1 Rural Village in a Sparse Setting E2 Rural Hamlets and Isolated Dwellings F1 Rural Hamlets and Isolated Dwellings in a Sparse Setting F2 In our calculation we group A1, B1, C1 and C2 as urban and any other code as rural. Joining the data The population table for the whole nation was joined onto the table with the urban/rural classification for each output area on the output area code \u201cOA11CD\u201d, hence bringing in the classification into the population data. Every OA and associated PWC is therefore categorised as either urban or rural. Later we use this classification to filter the data, and disaggregate our analysis. For the SDG analysis we remove the OAs classified as rural from our analysis, but we can choose to include them to expose transport availability in rural areas too. Computation of service areas As described in the methodology of the UN Metadata for this indicator, public transport service areas had to be calculated. Two methods to calculate service areas are described in the methodology: 1) using a Euclidean buffer to create polygon, and 2) to create a network/path calculation. Our team opted for the Euclidean buffer method for a number of reasons: We followed advice from the ONS Geospatial department that the simpler buffering method would be adequate for our needs Computationally Euclidean is would be much less resource intensive and does not require distributed compute power, whereas a network calculation would require scaled compute power. Other countries (such as Estonia , Norway and others) used the Euclidean buffer method and published their results. Our results will be more comparable with theirs Research shows that the network enquiry requires a complete path network, (as pointed out by Sweden in their write up, see section \u201c Result from the network distance calculations\u201d ) Euclidean buffering methodology We use Geopandas and for all geospatial operations in our analysis and the buffering operation is actually carried out by the Shapely object.buffer() function. This operation takes a geospatial point, and uses a radius of a given length to creates a polygon around the point which approximates a circle. The standard buffering of a point yields a polygon with 99.8% of the area of the circular disk it approximates. [ Figure 1: Illustration of the process of buffering a geospatial Point The resulting geospatial polygons are then joined and can be used for further calculations. Figure 2: Process of combining polygons to create the service area Notes on the network query method A network query would be calculated by taking paths of a specified length (500m or 1km) in every direction from a specified point; for this project that point would be a transport stop or station. Following these paths for the specified distance would create many end points. Finally end points are joined to create a perimeter, within which lies the service area. [ Figure 3: An example visualisation of a network distance calculation, taken from the Swedish methodology write up at https://www.efgs.info/11-2-1-sweden/ . The image shows the stops in blue, surrounded by the Euclidean buffer, shaded in green and a 500m limit shown. Calculation of population within service areas With the service areas calculated, the population that resides within a service area is calculated by a two stage process: A points in polygons enquiry which then filters the PWCs, so that the data set contains only those which are within the service areas, The population figure (number of individuals) associated with each PWC is summed, meaning that only the population within the service areas is counted as the population outside of the service areas was filtered out at stage 1. The proportion of the population inside a service area is calculated as a proportion of the total population. Currently for version 1.0, this is carried out at Local Authority (LA) level. Disaggregations As required for the SDG indicator we are producing this data for, the output data from this project has been disaggregated by sex, age and disability status. Shortcomings of the disaggregation method Our team recognises that in our current method, ethnicity is not a disaggregation that we use. At this stage for version 1.0 we are attempting to output data called for by the methodology in the UN Metadata . We regret that this important disaggregation is not included however, so our team intend to include this additional disaggregation in version 1.1 as an enhancement above what the original methodology requires. Disaggregating on other protected characteristics, as well as deprivation levels may be considered too. Disability status Classification and calculation of people with disabilities We classify disability using data from the ONS UK census, which is consistent with GSS harmonized disability data. To understand the data, we looked at the questions and their possible responses in the Measuring disability for the Equality Act 2010 harmonisation guidance . The questions are as follows: Question Response options Do you have any physical or mental health conditions or illnesses lasting or expected to last 12 months or more? Yes / No Does your condition or illness\\do any of your conditions or illnesses reduce your ability to carry-out day-to-day activities? Yes, a lot / Yes, a little / No The guidance states that the persons meeting the following criteria include: \"_A person who says yes, they have physical or mental health condition(s) or illness(es) lasting or expected to last for 12 months or more, but it doesn\u2019t restrict their activity are non-disabled._\" Therefore in our calculation, people will be considered \"Non-disabled\" if they: answer no to the first question answer yes to the first question, but no to the second question. And the calculations are as follows: Dtot = Dlot + Dlit I.e. \"Total people with disabilities\" = \"Day-to-day activities limited a lot\" + \"Day-to-day activities limited a little\" Or, in our Python code disab_total = disab_ltd_little + disab_ltd_lot Where disab_ltd_little and disab_ltd_lot are each column or Pandas Series. Then the total of non disabled people given by \"Non-disabled\" = \"Total Population\" - \"Total people with disabilities\" Or in our Python code non-disabled = pop_count - disab_total Considerations on disability status Our team has discussed options for counting individuals as either disabled or not. This is a complex and important area, and we recognise the importance of getting this as accurate as possible, as it may highlight areas in which those with disabilities are more affected by transport accessibility issues. Distance Standard distances of 500m and 1km are applied as a radius around the transport access nodes in order to create the public transport service areas. We speculate that these distances likely do not represent an accessible distance for many people however- this might include wheelchair users, elderly people and families with young children. Definitions We have opted to use a GSS Harmonised definition of disability for our analysis and the data comes from the census as described above. On the other hand the UN Metadata defines additional criteria to categorise public transport as conveniently accessible or not: \u201cPublic transport accessible to all special-needs customers, including those who are physically, visually, and/or hearing-impaired, as well as those with temporary disabilities, the elderly, children and other people in vulnerable situations\u201d_ In our analysis we are including the entire population, however, in our disaggregations we do not create a \u201cspecial-needs\u201d group. If we were to create such a group we should include people with temporary disabilities (if the data on this can be sourced), and the elderly or children. This has been proposed for version 1.2 of this project. Selection of age bins Population data was broken down by age on a year-by-year basis, from ages 0 through to 99. Rather than reporting the data or even calculating transport availability for every year in an age range, we opted to run the calculation for ages binned in 5-year brackets. We found no standard way to group ages. In other indicators across the SDG platform we found data grouped by age in various age increments and ranges, as seen in the disaggregation report for age . We selected the 5-year brackets (0-4, 5-9, 10-14 etc) to be similar to other UK indicators such as 3.3.3 and 3.4.1 among others. However we realise there are many other indicators which are not grouped in this way. If another age binning method is required, we plan to make our age_binning function more configurable (in version 1.2 of the project) so it will take population data by age and aggregate it into bins group by ages provided a bin_size parameter. This means that if the age binning needs to be changed so that, for instance, it can be compared to another dataset, the list of age groups can be changed easily and the analysis rerun. Aggregation and reporting To establish whether public stops and stations were within reach of people's places of residence the data analysis needed to be carried out at the most granular level possible, the output area level. However there are 175,434 output areas in England and Wales, so this would be too many data points to report on the data platform. Instead we aggregate up to large areas by request of our data end-user, the SDG data team. We aggregate our analysis up to the local authority (LA) level, and output areas fit perfectly within their parent local authority. The results aggregated in this way will display well on the UK SDG data platform , as it is well for developed to take this kind of geographical data!","title":"Methodology"},{"location":"methodology/#calculation-methodology-of-indicator-1121","text":"Version 1.0 James Westwood, Antonio Felton, Paige Hunter and Nathan Shaw.","title":"Calculation Methodology of Indicator 11.2.1"},{"location":"methodology/#introduction","text":"Our team has calculated data for transport accessibility across the UK following, as closely as we could, the method in the UN Metadata for the SDG indicator 11.2.1 . The following is a write up of the methodology we employed to make these calculations. In this methodology writeup we aim to accurately reflect the method used in version 1.0 of our project; read more about versions below in the section \u201cProject Versions\u201d.","title":"Introduction"},{"location":"methodology/#acknowledgement","text":"The main support for this project on both methodology and finding appropriate data sources has come from the geospatial department, in particular Musa Chirikeni, to whom we are very grateful. Other help has come from Michael Hodge who laid out the initial method we might approach in Python, and from the SDG data team who have been supportive data end users. We also thank Mark Simons who contributed to data management code in early 2022.","title":"Acknowledgement"},{"location":"methodology/#background","text":"The statistics calculated for this project are to be used as an indicator of the UK\u2019s progress on target 11.2 of the Sustainable Development Goals (SDGs). The SDGs are a set of 17 Goals which 193 Member states signed up to in order to coordinate efforts on more sustainable and inclusive national development. The goals cover areas such as poverty, hunger, equality and the environment. Goal 11, which the indicator we are calculating for falls under, is concerned with Sustainable Cities. The data our team produces will be available on the UK data platform for the SDG indicators , specifically on this page . Project history The project was initiated in late 2019 by James Westwood with some initial help from the UK SDG data team and Michael Hodge from the Data Science Campus, who provided guidance on the tech that would need to be used and a rough step-by-step calculation process. Project Versions The project is managed using version control on the Github platform. One feature of the version controlling using Github is that we can make releases. At the time of writing we have not made any releases of the code, but when we feel that all features to calculate the statistics required by the end user have been written into our code, we will release version 1.0. Each version has a project board, wherein we group the issues describing the work that needs to be done to develop each feature required. Project boards are hosted on Github. Version Focus Example features 1.0 Fully working reproducible calculation of transport availability across the UK Calculate the proportion of the population within 500m of a public transport access point for the whole of the UK. Disaggregate the above number by sex, age, disability status Be auditable and reproducable 1.1 Quality Assurance Phase Updates to code from feedback (e.g. from data team and topic expert) Data cleaning on import Validation on import Unit tests for functions 1.2 Enhanced functions and calculation Focus on enhancements to the calculation - Improve existing functions, making them more generic and robust - Look into improving geographical accuracy with more granular calculations - Improve the analysis with more disaggregations. 1.3 Optimise Computation Focus on enhancements to the functioning of the code: Better data management (e.g. SQL tables) Refactoring code Code optimization (speed, memory) vectorised calculation,","title":"Background"},{"location":"methodology/#calculation-process","text":"","title":"Calculation process"},{"location":"methodology/#process-diagram","text":"","title":"Process diagram"},{"location":"methodology/#main-method-of-calculation","text":"The key step of calculation is a spatial join between geolocated urban population data and public transport service areas. A discussion of how each component of that join was calculated from their component parts follows.","title":"Main method of calculation"},{"location":"methodology/#geocoding-the-population-data","text":"The questions we are trying to answer in this analysis is \u201cWhat proportion of people in the UK have access to public transport access points from their home?\u201d. And then the same question for people in each age, sex and disability category. To make the calculation as accurate as possible, we sourced the most granularly geolocated population data possible. Ideally, this would have been right down to the individual place of living (house/flat etc.) but such granularity is not available publicly, as it would be disclosive of individuals\u2019 living situations. The most granular level published that we are currently aware of is the \u201coutput area\u201d (OA) data, which derives from the census. Output areas (more fully defined and discussed in the data section of this methodology write up) are a conglomerate of the postcodes. Those postcodes are joined together on the basis that they are: Adjacent Of the same urban/rural classification In order to calculate the distance from the place any person lives, to a public transport access node (station or stop) we need a point-geometry for their residence. As we cannot geolocate individual residencies, we used an approximation in the form of population weighted centroids (PWCs). PWCs are a population-weighted location (x,y point) at which is a geospatial mean of the locations of all residences in that output area. The PWC, while extremely helpful in giving us a geospatial point to define where the population of any output area lives, is an approximation and is only used due to limitations in our data.","title":"Geocoding the population data"},{"location":"methodology/#joining-the-data","text":"Population weighted centroid (PWC) data is geolocated with x,y coordinates and has a common data column with the output area data (output area code, \u201cOA11CD\u201d) so a table join can be performed on those datasets. Then the entire population of any output area is approximated to live at the centroid.","title":"Joining the data"},{"location":"methodology/#limitations-and-improvements","text":"Our team recognises that PWCs do not accurately represent the location of any individual\u2019s place of residence and we intend to research methods of making calculations at even more granular levels, such as postcodes or smaller. Currently however we agreed with the our data end user and the geospatial department that the method to be used for version 1.0 will use the described approximation to geolocate the population. Delimitation of urban areas As Target 11 is concerned with sustainable urban environments, we ultimately had to select only urban areas and exclude rural areas from our analysis. Urban areas are defined as Ordnance Survey mapping that have resident populations above 10,000 people (2011 Census) and the methodology is available here . The urban/rural lookup data was sourced from ONS\u2019s Geography Portal and it provides a classification of each output area as either urban or rural categories which are further subdivided as follows. Classification Description Code Urban Major Conurbation A1 Urban Minor Conurbation B1 Urban City and Town C1 Urban City and Town in a Sparse Setting C2 Rural Town and Fringe D1 Rural Town and Fringe in a Sparse Setting D2 Rural Village E1 Rural Village in a Sparse Setting E2 Rural Hamlets and Isolated Dwellings F1 Rural Hamlets and Isolated Dwellings in a Sparse Setting F2 In our calculation we group A1, B1, C1 and C2 as urban and any other code as rural.","title":"Limitations and improvements"},{"location":"methodology/#joining-the-data_1","text":"The population table for the whole nation was joined onto the table with the urban/rural classification for each output area on the output area code \u201cOA11CD\u201d, hence bringing in the classification into the population data. Every OA and associated PWC is therefore categorised as either urban or rural. Later we use this classification to filter the data, and disaggregate our analysis. For the SDG analysis we remove the OAs classified as rural from our analysis, but we can choose to include them to expose transport availability in rural areas too.","title":"Joining the data"},{"location":"methodology/#computation-of-service-areas","text":"As described in the methodology of the UN Metadata for this indicator, public transport service areas had to be calculated. Two methods to calculate service areas are described in the methodology: 1) using a Euclidean buffer to create polygon, and 2) to create a network/path calculation. Our team opted for the Euclidean buffer method for a number of reasons: We followed advice from the ONS Geospatial department that the simpler buffering method would be adequate for our needs Computationally Euclidean is would be much less resource intensive and does not require distributed compute power, whereas a network calculation would require scaled compute power. Other countries (such as Estonia , Norway and others) used the Euclidean buffer method and published their results. Our results will be more comparable with theirs Research shows that the network enquiry requires a complete path network, (as pointed out by Sweden in their write up, see section \u201c Result from the network distance calculations\u201d )","title":"Computation of service areas"},{"location":"methodology/#euclidean-buffering-methodology","text":"We use Geopandas and for all geospatial operations in our analysis and the buffering operation is actually carried out by the Shapely object.buffer() function. This operation takes a geospatial point, and uses a radius of a given length to creates a polygon around the point which approximates a circle. The standard buffering of a point yields a polygon with 99.8% of the area of the circular disk it approximates. [ Figure 1: Illustration of the process of buffering a geospatial Point The resulting geospatial polygons are then joined and can be used for further calculations. Figure 2: Process of combining polygons to create the service area","title":"Euclidean buffering methodology"},{"location":"methodology/#notes-on-the-network-query-method","text":"A network query would be calculated by taking paths of a specified length (500m or 1km) in every direction from a specified point; for this project that point would be a transport stop or station. Following these paths for the specified distance would create many end points. Finally end points are joined to create a perimeter, within which lies the service area. [ Figure 3: An example visualisation of a network distance calculation, taken from the Swedish methodology write up at https://www.efgs.info/11-2-1-sweden/ . The image shows the stops in blue, surrounded by the Euclidean buffer, shaded in green and a 500m limit shown.","title":"Notes on the network query method"},{"location":"methodology/#calculation-of-population-within-service-areas","text":"With the service areas calculated, the population that resides within a service area is calculated by a two stage process: A points in polygons enquiry which then filters the PWCs, so that the data set contains only those which are within the service areas, The population figure (number of individuals) associated with each PWC is summed, meaning that only the population within the service areas is counted as the population outside of the service areas was filtered out at stage 1. The proportion of the population inside a service area is calculated as a proportion of the total population. Currently for version 1.0, this is carried out at Local Authority (LA) level.","title":"Calculation of population within service areas"},{"location":"methodology/#disaggregations","text":"As required for the SDG indicator we are producing this data for, the output data from this project has been disaggregated by sex, age and disability status.","title":"Disaggregations"},{"location":"methodology/#shortcomings-of-the-disaggregation-method","text":"Our team recognises that in our current method, ethnicity is not a disaggregation that we use. At this stage for version 1.0 we are attempting to output data called for by the methodology in the UN Metadata . We regret that this important disaggregation is not included however, so our team intend to include this additional disaggregation in version 1.1 as an enhancement above what the original methodology requires. Disaggregating on other protected characteristics, as well as deprivation levels may be considered too.","title":"Shortcomings of the disaggregation method"},{"location":"methodology/#disability-status","text":"","title":"Disability status"},{"location":"methodology/#classification-and-calculation-of-people-with-disabilities","text":"We classify disability using data from the ONS UK census, which is consistent with GSS harmonized disability data. To understand the data, we looked at the questions and their possible responses in the Measuring disability for the Equality Act 2010 harmonisation guidance . The questions are as follows: Question Response options Do you have any physical or mental health conditions or illnesses lasting or expected to last 12 months or more? Yes / No Does your condition or illness\\do any of your conditions or illnesses reduce your ability to carry-out day-to-day activities? Yes, a lot / Yes, a little / No The guidance states that the persons meeting the following criteria include: \"_A person who says yes, they have physical or mental health condition(s) or illness(es) lasting or expected to last for 12 months or more, but it doesn\u2019t restrict their activity are non-disabled._\" Therefore in our calculation, people will be considered \"Non-disabled\" if they: answer no to the first question answer yes to the first question, but no to the second question. And the calculations are as follows: Dtot = Dlot + Dlit I.e. \"Total people with disabilities\" = \"Day-to-day activities limited a lot\" + \"Day-to-day activities limited a little\" Or, in our Python code disab_total = disab_ltd_little + disab_ltd_lot Where disab_ltd_little and disab_ltd_lot are each column or Pandas Series. Then the total of non disabled people given by \"Non-disabled\" = \"Total Population\" - \"Total people with disabilities\" Or in our Python code non-disabled = pop_count - disab_total","title":"Classification and calculation of people with disabilities"},{"location":"methodology/#considerations-on-disability-status","text":"Our team has discussed options for counting individuals as either disabled or not. This is a complex and important area, and we recognise the importance of getting this as accurate as possible, as it may highlight areas in which those with disabilities are more affected by transport accessibility issues.","title":"Considerations on disability status"},{"location":"methodology/#distance","text":"Standard distances of 500m and 1km are applied as a radius around the transport access nodes in order to create the public transport service areas. We speculate that these distances likely do not represent an accessible distance for many people however- this might include wheelchair users, elderly people and families with young children.","title":"Distance"},{"location":"methodology/#definitions","text":"We have opted to use a GSS Harmonised definition of disability for our analysis and the data comes from the census as described above. On the other hand the UN Metadata defines additional criteria to categorise public transport as conveniently accessible or not: \u201cPublic transport accessible to all special-needs customers, including those who are physically, visually, and/or hearing-impaired, as well as those with temporary disabilities, the elderly, children and other people in vulnerable situations\u201d_ In our analysis we are including the entire population, however, in our disaggregations we do not create a \u201cspecial-needs\u201d group. If we were to create such a group we should include people with temporary disabilities (if the data on this can be sourced), and the elderly or children. This has been proposed for version 1.2 of this project.","title":"Definitions"},{"location":"methodology/#selection-of-age-bins","text":"Population data was broken down by age on a year-by-year basis, from ages 0 through to 99. Rather than reporting the data or even calculating transport availability for every year in an age range, we opted to run the calculation for ages binned in 5-year brackets. We found no standard way to group ages. In other indicators across the SDG platform we found data grouped by age in various age increments and ranges, as seen in the disaggregation report for age . We selected the 5-year brackets (0-4, 5-9, 10-14 etc) to be similar to other UK indicators such as 3.3.3 and 3.4.1 among others. However we realise there are many other indicators which are not grouped in this way. If another age binning method is required, we plan to make our age_binning function more configurable (in version 1.2 of the project) so it will take population data by age and aggregate it into bins group by ages provided a bin_size parameter. This means that if the age binning needs to be changed so that, for instance, it can be compared to another dataset, the list of age groups can be changed easily and the analysis rerun.","title":"Selection of age bins"},{"location":"methodology/#aggregation-and-reporting","text":"To establish whether public stops and stations were within reach of people's places of residence the data analysis needed to be carried out at the most granular level possible, the output area level. However there are 175,434 output areas in England and Wales, so this would be too many data points to report on the data platform. Instead we aggregate up to large areas by request of our data end-user, the SDG data team. We aggregate our analysis up to the local authority (LA) level, and output areas fit perfectly within their parent local authority. The results aggregated in this way will display well on the UK SDG data platform , as it is well for developed to take this kind of geographical data!","title":"Aggregation and reporting"},{"location":"time_table_utils/","text":"Technical documentation for the time_table_utils module. Any docstrings in this file are automatically copied to this page. All functions realted to the bus and train timetable data. add_stop_capacity_type ( stops_df ) Adds capacity_type column. Column is defined with the following dictionary using the StopType Bus stops are low capacity, train stations are high capacity. Parameters: stops_df ( pd . DataFrame ) \u2013 The dataframe to add the column to. Returns: pd . DataFrame \u2013 pd.DataFrame: dataframe with new capacity_type column. src/time_table/time_table_utils.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def add_stop_capacity_type ( stops_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Adds capacity_type column. Column is defined with the following dictionary using the StopType Bus stops are low capacity, train stations are high capacity. Args: stops_df (pd.DataFrame): The dataframe to add the column to. Returns: pd.DataFrame: dataframe with new capacity_type column. \"\"\" # Create a dictionary to map the StopType to capacity level capacity_map = { \"RSE\" : \"high\" , \"RLY\" : \"high\" , \"RPL\" : \"high\" , \"TMU\" : \"high\" , \"MET\" : \"high\" , \"PLT\" : \"high\" , \"BCE\" : \"low\" , \"BST\" : \"low\" , \"BCQ\" : \"low\" , \"BCS\" : \"low\" , \"BCT\" : \"low\" } # Add the capacity_type column to the stops dataframe stops_df [ \"capacity_type\" ] = stops_df [ \"StopType\" ] . map ( capacity_map ) return stops_df extract_mca ( mca_file ) Extract data from the mca file. The logic for this extraction is as follows Each new journey starts with \"BS\". Within this journey we have * multiple stops * LO is origin * LI are inbetween stops * LT is terminating stop * Then a new journey starts with BS again Within each journey are a few more lines that we can ignore e.g. * BX = extra details of the journey * CR = changes en route. Doesnt contain any arrival / departure times. Process: * Starts by finding all the schedules within the file. * Extract relevant information into the journey dataframe, and then copy unique_id * onto all trips within that journey. Parameters: mca_file ( _type_ ) \u2013 description Returns: schedules ( list ) \u2013 list of lists containing schedule information ready for dataframe stops ( list ) \u2013 list of lists containing stop information ready for dataframe src/time_table/time_table_utils.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def extract_mca ( mca_file : str ) -> Tuple [ List [ List ], List [ List ]]: \"\"\"Extract data from the mca file. The logic for this extraction is as follows: Each new journey starts with \"BS\". Within this journey we have * multiple stops * LO is origin * LI are inbetween stops * LT is terminating stop * Then a new journey starts with BS again Within each journey are a few more lines that we can ignore e.g. * BX = extra details of the journey * CR = changes en route. Doesnt contain any arrival / departure times. Process: * Starts by finding all the schedules within the file. * Extract relevant information into the journey dataframe, and then copy unique_id * onto all trips within that journey. Args: mca_file (_type_): _description_ Returns: schedules (list): list of lists containing schedule information ready for dataframe stops (list): list of lists containing stop information ready for dataframe \"\"\" # Create a flag that specifies when we have found a new journey journey = False # Store schedule information schedules = [] # Store stop information stops = [] with open ( mca_file , 'r' ) as mca_data : # Skip the header next ( mca_data ) for line in mca_data : # A schedule is started by a record beginning with BS. # Other entries exist but are not needed for our purpose. # Schedules are then further broken down by transaction type # (N - new, R - revised, D - delete) # Ignore any that are transaction type delete. if line [ 0 : 3 ] == 'BSN' or line [ 0 : 3 ] == 'BSR' : # Switch flag on as we have found a journey journey = True # Get unique ID for schedule # ID in dataset is not actually unique as same train has several # schedules with different dates and calendars. Create ID from # these variables. schedule_id = ( line [ 3 : 9 ] + line [ 9 : 15 ] + line [ 15 : 21 ] + line [ 21 : 28 ]) # Extract start and end date of service (yymmdd) start_date = line [ 9 : 15 ] . strip () end_date = line [ 15 : 21 ] . strip () # Extract the calender information for this journey # i.e. what days of the week it runs # Only interested in weekdays for the timebeing. monday = int ( line [ 21 ] . strip ()) tuesday = int ( line [ 22 ] . strip ()) wednesday = int ( line [ 23 ] . strip ()) thursday = int ( line [ 24 ] . strip ()) friday = int ( line [ 25 ] . strip ()) # Store data to be added to the dataframe schedules . append ([ schedule_id , start_date , end_date , monday , tuesday , wednesday , thursday , friday ]) # Skip as this is all we need to do for an entry starting # with BS continue # If we have found a new journey, go through the lines that are # part of this jounrey and give them the schedule id. # Journeys split into origin (LO), stops (LI) and terminus (LT) # CR (changes en route) and BX (extra schedule details) can be # ignored as not relevant to our purpose. # If station has a departure time extract time, tiploc_code and # activity type. # NB times can end on a H sometimes which indicates a half minute # Rather than rounding up and down, just ignoring this for the moment # and taking only first four characters (hh:mm) if journey : if line [ 0 : 2 ] == 'LO' : departure_time = line [ 10 : 14 ] . strip () tiploc_code = line [ 2 : 10 ] . strip () activity_type = line [ 29 : 41 ] . strip () elif line [ 0 : 2 ] == 'LI' : departure_time = line [ 15 : 19 ] . strip () tiploc_code = line [ 2 : 10 ] . strip () activity_type = line [ 42 : 54 ] . strip () elif line [ 0 : 2 ] == 'LT' : departure_time = line [ 15 : 19 ] . strip () tiploc_code = line [ 2 : 10 ] . strip () activity_type = line [ 25 : 37 ] . strip () # As we know that LT signifies the last stop in a journey # and we have extracted everything we need we now switch the # flag off journey = False else : # Skipping BR and CX continue # Store data to be added to dataframe stops . append ([ schedule_id , departure_time , tiploc_code , activity_type ]) return schedules , stops extract_msn_data ( msn_file ) Extract data from the msn file. Parameters: msn_file ( msn ) \u2013 A text file containing the msn data. Returns: list ( List [ List ] ) \u2013 A list of lists containing the msn data. src/time_table/time_table_utils.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def extract_msn_data ( msn_file : str ) -> List [ List ]: \"\"\"Extract data from the msn file. Args: msn_file (msn): A text file containing the msn data. Returns: list: A list of lists containing the msn data. \"\"\" # Store msn data msn_data_lst = [] with open ( msn_file , 'r' ) as msn_data : # Skip header next ( msn_data ) for line in msn_data : # Only interested in rows starting with A. # Rows starting with L display aliases of station names # Stripping the values because some are padded out with blank # spaces as part of the file format. # Coordinate data provided is actually the grid reference # but without the 100km square (two letters at the start) so # very difficult to extract coordinates. Hence, will add in # coordinate data from an external source. # NB tiploc_code is unique, but crs_code isnt. if line . startswith ( 'A' ): station_name = line [ 5 : 31 ] . strip () tiploc_code = line [ 36 : 43 ] . strip () crs_code = line [ 49 : 52 ] . strip () msn_data_lst . append ([ station_name , tiploc_code , crs_code ]) return msn_data_lst filter_stops ( stops_df ) Filters the stops dataframe based on two things: | 1) Status column. We want to keep stops which are active, pending or new. | 2) StopType want only to include bus and rail stops. Parameters: stops_df ( pd . DataFrame ) \u2013 the dataframe to filter. Returns: pd . DataFrame \u2013 pd.DataFrame: Filtered_stops which meet the criteria of keeping based on status/stoptype columns. src/time_table/time_table_utils.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def filter_stops ( stops_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Filters the stops dataframe based on two things: | 1) Status column. We want to keep stops which are active, pending or new. | 2) StopType want only to include bus and rail stops. Args: stops_df (pd.DataFrame): the dataframe to filter. Returns: pd.DataFrame: Filtered_stops which meet the criteria of keeping based on status/stoptype columns. \"\"\" # stop_types we would like to keep within the dataframe stop_types = [ \"RSE\" , \"RLY\" , \"RPL\" , \"TMU\" , \"MET\" , \"PLT\" , \"BCE\" , \"BST\" , \"BCQ\" , \"BCS\" , \"BCT\" ] # Filter the stops based on the status column (active, pending, new and # None) filtered_stops = stops_df [( stops_df [ \"Status\" ] == \"active\" ) | ( stops_df [ \"Status\" ] == \"pending\" ) | ( stops_df [ \"Status\" ] is None ) | ( stops_df [ \"Status\" ] == \"new\" )] # Filter the stops based on the stop types (bus and rail) boolean_stops_type = filtered_stops [ \"StopType\" ] . isin ( stop_types ) filter_stops = filtered_stops [ boolean_stops_type ] return filter_stops filter_timetable_by_day ( timetable_df , day ) Extract serviced stops based on specific day of the week. The day is selected from the available days in the date range present in timetable data. 1) identifies which days dates in the entire date range 2) counts days of each type to get the maximum position order 3) validates user's choice for day - provides useful errors 4) creates ord value that is half of maximum position order to ensure as many services get included as possible. 4) selects a date based on the day and ord parameters 5) filters the dataframe to that date Parameters: timetable_df ( pandas dataframe ) \u2013 df to filter day ( str) ) \u2013 day of the week in title case, e.g. \"Wednesday\" Returns: pd . DataFrame \u2013 pd.DataFrame: filtered pandas dataframe src/time_table/time_table_utils.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def filter_timetable_by_day ( timetable_df : pd . DataFrame , day : str ) -> pd . DataFrame : \"\"\"Extract serviced stops based on specific day of the week. The day is selected from the available days in the date range present in timetable data. 1) identifies which days dates in the entire date range 2) counts days of each type to get the maximum position order 3) validates user's choice for `day` - provides useful errors 4) creates ord value that is half of maximum position order to ensure as many services get included as possible. 4) selects a date based on the day and ord parameters 5) filters the dataframe to that date Args: timetable_df (pandas dataframe): df to filter day (str) : day of the week in title case, e.g. \"Wednesday\" Returns: pd.DataFrame: filtered pandas dataframe \"\"\" # Measure the dataframe original_rows = timetable_df . shape [ 0 ] # Count the services orig_service_count = timetable_df . service_id . unique () . shape [ 0 ] # Get the minimum date range earliest_start_date = timetable_df . start_date . min () latest_end_date = timetable_df . end_date . max () # Identify days in the range and count them date_range = pd . date_range ( earliest_start_date , latest_end_date ) date_day_couplings_df = pd . DataFrame ({ \"date\" : date_range , \"day_name\" : date_range . day_name ()}) days_counted = date_day_couplings_df . day_name . value_counts () days_counted_dict = days_counted . to_dict () # Validate user choices if day not in days_counted_dict . keys (): raise KeyError ( \"\"\"The day chosen in not available. Should be a weekday in title case.\"\"\" ) # Get the maximum position order (ordinal) max_ord = days_counted_dict [ day ] ord = round ( max_ord / 2 ) # Filter all the dates down the to the day needed day_filtered_dates = ( date_day_couplings_df [ date_day_couplings_df . day_name == day ]) # Get date of the nth (ord) day nth = ord - 1 date_of_day_entered = day_filtered_dates . iloc [ nth ] . date # Filter the timetable_df by date range timetable_df = timetable_df [( timetable_df [ 'start_date' ] <= date_of_day_entered ) & ( timetable_df [ 'end_date' ] >= date_of_day_entered )] # Then filter to day of interest timetable_df = timetable_df [ timetable_df [ day . lower ()] == 1 ] # Filter the timetable_df by date range timetable_df = timetable_df [( timetable_df [ 'start_date' ] <= date_of_day_entered ) & ( timetable_df [ 'end_date' ] >= date_of_day_entered )] # Then filter to day of interest timetable_df = timetable_df [ timetable_df [ day . lower ()] == 1 ] # Print date being used (consider logging instead) day_date = date_of_day_entered . date () logger ( f \"The date of { day } number { ord } is { day_date } \" ) # Print how many rows have been dropped (consider logging instead) logger ( f \"Selecting only services covering { day_date } reduced records\" f \"by { original_rows - timetable_df . shape [ 0 ] } rows\" ) # Print how many services are in the analysis and how many were dropped service_count = timetable_df . service_id . unique () . shape [ 0 ] dropped_services = orig_service_count - service_count logger ( f \"There are { service_count } services in the analysis\" ) logger ( f \"Filtering by day has reduced services by { dropped_services } \" ) return timetable_df","title":"Time table utils"},{"location":"time_table_utils/#src.time_table.time_table_utils.add_stop_capacity_type","text":"Adds capacity_type column. Column is defined with the following dictionary using the StopType Bus stops are low capacity, train stations are high capacity. Parameters: stops_df ( pd . DataFrame ) \u2013 The dataframe to add the column to. Returns: pd . DataFrame \u2013 pd.DataFrame: dataframe with new capacity_type column. src/time_table/time_table_utils.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def add_stop_capacity_type ( stops_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Adds capacity_type column. Column is defined with the following dictionary using the StopType Bus stops are low capacity, train stations are high capacity. Args: stops_df (pd.DataFrame): The dataframe to add the column to. Returns: pd.DataFrame: dataframe with new capacity_type column. \"\"\" # Create a dictionary to map the StopType to capacity level capacity_map = { \"RSE\" : \"high\" , \"RLY\" : \"high\" , \"RPL\" : \"high\" , \"TMU\" : \"high\" , \"MET\" : \"high\" , \"PLT\" : \"high\" , \"BCE\" : \"low\" , \"BST\" : \"low\" , \"BCQ\" : \"low\" , \"BCS\" : \"low\" , \"BCT\" : \"low\" } # Add the capacity_type column to the stops dataframe stops_df [ \"capacity_type\" ] = stops_df [ \"StopType\" ] . map ( capacity_map ) return stops_df","title":"add_stop_capacity_type()"},{"location":"time_table_utils/#src.time_table.time_table_utils.extract_mca","text":"Extract data from the mca file. The logic for this extraction is as follows Each new journey starts with \"BS\". Within this journey we have * multiple stops * LO is origin * LI are inbetween stops * LT is terminating stop * Then a new journey starts with BS again Within each journey are a few more lines that we can ignore e.g. * BX = extra details of the journey * CR = changes en route. Doesnt contain any arrival / departure times. Process: * Starts by finding all the schedules within the file. * Extract relevant information into the journey dataframe, and then copy unique_id * onto all trips within that journey. Parameters: mca_file ( _type_ ) \u2013 description Returns: schedules ( list ) \u2013 list of lists containing schedule information ready for dataframe stops ( list ) \u2013 list of lists containing stop information ready for dataframe src/time_table/time_table_utils.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def extract_mca ( mca_file : str ) -> Tuple [ List [ List ], List [ List ]]: \"\"\"Extract data from the mca file. The logic for this extraction is as follows: Each new journey starts with \"BS\". Within this journey we have * multiple stops * LO is origin * LI are inbetween stops * LT is terminating stop * Then a new journey starts with BS again Within each journey are a few more lines that we can ignore e.g. * BX = extra details of the journey * CR = changes en route. Doesnt contain any arrival / departure times. Process: * Starts by finding all the schedules within the file. * Extract relevant information into the journey dataframe, and then copy unique_id * onto all trips within that journey. Args: mca_file (_type_): _description_ Returns: schedules (list): list of lists containing schedule information ready for dataframe stops (list): list of lists containing stop information ready for dataframe \"\"\" # Create a flag that specifies when we have found a new journey journey = False # Store schedule information schedules = [] # Store stop information stops = [] with open ( mca_file , 'r' ) as mca_data : # Skip the header next ( mca_data ) for line in mca_data : # A schedule is started by a record beginning with BS. # Other entries exist but are not needed for our purpose. # Schedules are then further broken down by transaction type # (N - new, R - revised, D - delete) # Ignore any that are transaction type delete. if line [ 0 : 3 ] == 'BSN' or line [ 0 : 3 ] == 'BSR' : # Switch flag on as we have found a journey journey = True # Get unique ID for schedule # ID in dataset is not actually unique as same train has several # schedules with different dates and calendars. Create ID from # these variables. schedule_id = ( line [ 3 : 9 ] + line [ 9 : 15 ] + line [ 15 : 21 ] + line [ 21 : 28 ]) # Extract start and end date of service (yymmdd) start_date = line [ 9 : 15 ] . strip () end_date = line [ 15 : 21 ] . strip () # Extract the calender information for this journey # i.e. what days of the week it runs # Only interested in weekdays for the timebeing. monday = int ( line [ 21 ] . strip ()) tuesday = int ( line [ 22 ] . strip ()) wednesday = int ( line [ 23 ] . strip ()) thursday = int ( line [ 24 ] . strip ()) friday = int ( line [ 25 ] . strip ()) # Store data to be added to the dataframe schedules . append ([ schedule_id , start_date , end_date , monday , tuesday , wednesday , thursday , friday ]) # Skip as this is all we need to do for an entry starting # with BS continue # If we have found a new journey, go through the lines that are # part of this jounrey and give them the schedule id. # Journeys split into origin (LO), stops (LI) and terminus (LT) # CR (changes en route) and BX (extra schedule details) can be # ignored as not relevant to our purpose. # If station has a departure time extract time, tiploc_code and # activity type. # NB times can end on a H sometimes which indicates a half minute # Rather than rounding up and down, just ignoring this for the moment # and taking only first four characters (hh:mm) if journey : if line [ 0 : 2 ] == 'LO' : departure_time = line [ 10 : 14 ] . strip () tiploc_code = line [ 2 : 10 ] . strip () activity_type = line [ 29 : 41 ] . strip () elif line [ 0 : 2 ] == 'LI' : departure_time = line [ 15 : 19 ] . strip () tiploc_code = line [ 2 : 10 ] . strip () activity_type = line [ 42 : 54 ] . strip () elif line [ 0 : 2 ] == 'LT' : departure_time = line [ 15 : 19 ] . strip () tiploc_code = line [ 2 : 10 ] . strip () activity_type = line [ 25 : 37 ] . strip () # As we know that LT signifies the last stop in a journey # and we have extracted everything we need we now switch the # flag off journey = False else : # Skipping BR and CX continue # Store data to be added to dataframe stops . append ([ schedule_id , departure_time , tiploc_code , activity_type ]) return schedules , stops","title":"extract_mca()"},{"location":"time_table_utils/#src.time_table.time_table_utils.extract_msn_data","text":"Extract data from the msn file. Parameters: msn_file ( msn ) \u2013 A text file containing the msn data. Returns: list ( List [ List ] ) \u2013 A list of lists containing the msn data. src/time_table/time_table_utils.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def extract_msn_data ( msn_file : str ) -> List [ List ]: \"\"\"Extract data from the msn file. Args: msn_file (msn): A text file containing the msn data. Returns: list: A list of lists containing the msn data. \"\"\" # Store msn data msn_data_lst = [] with open ( msn_file , 'r' ) as msn_data : # Skip header next ( msn_data ) for line in msn_data : # Only interested in rows starting with A. # Rows starting with L display aliases of station names # Stripping the values because some are padded out with blank # spaces as part of the file format. # Coordinate data provided is actually the grid reference # but without the 100km square (two letters at the start) so # very difficult to extract coordinates. Hence, will add in # coordinate data from an external source. # NB tiploc_code is unique, but crs_code isnt. if line . startswith ( 'A' ): station_name = line [ 5 : 31 ] . strip () tiploc_code = line [ 36 : 43 ] . strip () crs_code = line [ 49 : 52 ] . strip () msn_data_lst . append ([ station_name , tiploc_code , crs_code ]) return msn_data_lst","title":"extract_msn_data()"},{"location":"time_table_utils/#src.time_table.time_table_utils.filter_stops","text":"Filters the stops dataframe based on two things: | 1) Status column. We want to keep stops which are active, pending or new. | 2) StopType want only to include bus and rail stops. Parameters: stops_df ( pd . DataFrame ) \u2013 the dataframe to filter. Returns: pd . DataFrame \u2013 pd.DataFrame: Filtered_stops which meet the criteria of keeping based on status/stoptype columns. src/time_table/time_table_utils.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def filter_stops ( stops_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Filters the stops dataframe based on two things: | 1) Status column. We want to keep stops which are active, pending or new. | 2) StopType want only to include bus and rail stops. Args: stops_df (pd.DataFrame): the dataframe to filter. Returns: pd.DataFrame: Filtered_stops which meet the criteria of keeping based on status/stoptype columns. \"\"\" # stop_types we would like to keep within the dataframe stop_types = [ \"RSE\" , \"RLY\" , \"RPL\" , \"TMU\" , \"MET\" , \"PLT\" , \"BCE\" , \"BST\" , \"BCQ\" , \"BCS\" , \"BCT\" ] # Filter the stops based on the status column (active, pending, new and # None) filtered_stops = stops_df [( stops_df [ \"Status\" ] == \"active\" ) | ( stops_df [ \"Status\" ] == \"pending\" ) | ( stops_df [ \"Status\" ] is None ) | ( stops_df [ \"Status\" ] == \"new\" )] # Filter the stops based on the stop types (bus and rail) boolean_stops_type = filtered_stops [ \"StopType\" ] . isin ( stop_types ) filter_stops = filtered_stops [ boolean_stops_type ] return filter_stops","title":"filter_stops()"},{"location":"time_table_utils/#src.time_table.time_table_utils.filter_timetable_by_day","text":"Extract serviced stops based on specific day of the week. The day is selected from the available days in the date range present in timetable data. 1) identifies which days dates in the entire date range 2) counts days of each type to get the maximum position order 3) validates user's choice for day - provides useful errors 4) creates ord value that is half of maximum position order to ensure as many services get included as possible. 4) selects a date based on the day and ord parameters 5) filters the dataframe to that date Parameters: timetable_df ( pandas dataframe ) \u2013 df to filter day ( str) ) \u2013 day of the week in title case, e.g. \"Wednesday\" Returns: pd . DataFrame \u2013 pd.DataFrame: filtered pandas dataframe src/time_table/time_table_utils.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def filter_timetable_by_day ( timetable_df : pd . DataFrame , day : str ) -> pd . DataFrame : \"\"\"Extract serviced stops based on specific day of the week. The day is selected from the available days in the date range present in timetable data. 1) identifies which days dates in the entire date range 2) counts days of each type to get the maximum position order 3) validates user's choice for `day` - provides useful errors 4) creates ord value that is half of maximum position order to ensure as many services get included as possible. 4) selects a date based on the day and ord parameters 5) filters the dataframe to that date Args: timetable_df (pandas dataframe): df to filter day (str) : day of the week in title case, e.g. \"Wednesday\" Returns: pd.DataFrame: filtered pandas dataframe \"\"\" # Measure the dataframe original_rows = timetable_df . shape [ 0 ] # Count the services orig_service_count = timetable_df . service_id . unique () . shape [ 0 ] # Get the minimum date range earliest_start_date = timetable_df . start_date . min () latest_end_date = timetable_df . end_date . max () # Identify days in the range and count them date_range = pd . date_range ( earliest_start_date , latest_end_date ) date_day_couplings_df = pd . DataFrame ({ \"date\" : date_range , \"day_name\" : date_range . day_name ()}) days_counted = date_day_couplings_df . day_name . value_counts () days_counted_dict = days_counted . to_dict () # Validate user choices if day not in days_counted_dict . keys (): raise KeyError ( \"\"\"The day chosen in not available. Should be a weekday in title case.\"\"\" ) # Get the maximum position order (ordinal) max_ord = days_counted_dict [ day ] ord = round ( max_ord / 2 ) # Filter all the dates down the to the day needed day_filtered_dates = ( date_day_couplings_df [ date_day_couplings_df . day_name == day ]) # Get date of the nth (ord) day nth = ord - 1 date_of_day_entered = day_filtered_dates . iloc [ nth ] . date # Filter the timetable_df by date range timetable_df = timetable_df [( timetable_df [ 'start_date' ] <= date_of_day_entered ) & ( timetable_df [ 'end_date' ] >= date_of_day_entered )] # Then filter to day of interest timetable_df = timetable_df [ timetable_df [ day . lower ()] == 1 ] # Filter the timetable_df by date range timetable_df = timetable_df [( timetable_df [ 'start_date' ] <= date_of_day_entered ) & ( timetable_df [ 'end_date' ] >= date_of_day_entered )] # Then filter to day of interest timetable_df = timetable_df [ timetable_df [ day . lower ()] == 1 ] # Print date being used (consider logging instead) day_date = date_of_day_entered . date () logger ( f \"The date of { day } number { ord } is { day_date } \" ) # Print how many rows have been dropped (consider logging instead) logger ( f \"Selecting only services covering { day_date } reduced records\" f \"by { original_rows - timetable_df . shape [ 0 ] } rows\" ) # Print how many services are in the analysis and how many were dropped service_count = timetable_df . service_id . unique () . shape [ 0 ] dropped_services = orig_service_count - service_count logger ( f \"There are { service_count } services in the analysis\" ) logger ( f \"Filtering by day has reduced services by { dropped_services } \" ) return timetable_df","title":"filter_timetable_by_day()"}]}